{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:56:25.290388821Z",
     "start_time": "2023-10-05T07:56:24.938184962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:24:59.323806965Z",
     "start_time": "2023-10-05T08:24:59.250097062Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:08:42.454578470Z",
     "start_time": "2023-10-05T08:08:42.414169284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((10000,), (10000, 2))"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:56:29.705331903Z",
     "start_time": "2023-10-05T07:56:29.692428271Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    nb_sample = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    return 1/(2*nb_sample) * e.T @ e\n",
    "    # ***************************************************x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def compute_loss_MAE(y, tx, w):\n",
    "    nb_sample = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    return 1/nb_sample * np.sum(np.abs(e))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T08:06:18.942034761Z",
     "start_time": "2023-10-05T08:06:18.773445232Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:56:31.315126303Z",
     "start_time": "2023-10-05T07:56:31.277181258Z"
    }
   },
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    w = np.hstack((grid_w0[:, np.newaxis], grid_w1[:, np.newaxis]))\n",
    "    for i in range(len(grid_w0)):\n",
    "        for j in range(len(grid_w1)):\n",
    "            w = np.array([grid_w0[i], grid_w1[j]])\n",
    "            losses[i, j] = compute_loss(y, tx, w)\n",
    "            \n",
    "    # ***************************************************\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:56:31.988903755Z",
     "start_time": "2023-10-05T07:56:31.972051900Z"
    }
   },
   "outputs": [],
   "source": [
    "# This function is a bit more efficient as it doesn't contain any for loop, but it is not possible to use the function compute_loss as\n",
    "# we implemented it, because it expects to treat each (w0, w1) separately\n",
    "\n",
    "# This one uses indices\n",
    "def grid_search_v2(y, tx, grid_w0, grid_w1):\n",
    "    row, col = np.indices((len(grid_w0), len(grid_w1)))\n",
    "    big_w0, big_w1 = grid_w0[row.ravel()], grid_w1[col.ravel()]\n",
    "    w = np.column_stack((big_w0, big_w1))\n",
    "    e = (y[:, np.newaxis] - tx @ w.T)**2\n",
    "    nb_sample = y.shape[0]\n",
    "    losses = 1/(2*nb_sample) * np.sum(e, axis=0)\n",
    "    return losses.reshape((len(grid_w0), len(grid_w1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:56:32.310394943Z",
     "start_time": "2023-10-05T07:56:32.286161632Z"
    }
   },
   "outputs": [],
   "source": [
    "# This one uses broadcasting, the broadcasting must be explicit, as we have to stack the arrays\n",
    "def grid_search_v3(y, tx, grid_w0, grid_w1):\n",
    "    w0_size, w1_size = grid_w0.shape[0], grid_w1.shape[0]\n",
    "    w0 = np.broadcast_to(grid_w0[:, np.newaxis], (w0_size, w1_size))\n",
    "    w1 = np.broadcast_to(grid_w1[np.newaxis, :], (w0_size, w1_size))\n",
    "    w = np.column_stack((w0.ravel(), w1.ravel()))\n",
    "    e = (y[:, np.newaxis] - tx @ w.T)**2\n",
    "    nb_sample = y.shape[0]\n",
    "    losses = 1/(2*nb_sample) * np.sum(e, axis=0)\n",
    "    return losses.reshape((len(grid_w0), len(grid_w1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:53:44.308274536Z",
     "start_time": "2023-10-05T07:53:20.292125293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4 s ± 2.88 s per loop (mean ± std. dev. of 3 runs, 3 loops each)\n",
      "6 s ± 164 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n",
      "6.12 s ± 121 ms per loop (mean ± std. dev. of 3 runs, 3 loops each)\n"
     ]
    }
   ],
   "source": [
    "from grid_search import generate_w\n",
    "grid_w0_t, grid_w1_t = generate_w(num_intervals=100)\n",
    "%timeit -r 3 -n 3 -c grid_search(y, tx, grid_w0_t, grid_w1_t)\n",
    "%timeit -r 3 -n 3 -c grid_search_v2(y, tx, grid_w0_t, grid_w1_t)\n",
    "%timeit -r 3 -n 3 -c grid_search_v3(y, tx, grid_w0_t, grid_w1_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:43:34.297882494Z",
     "start_time": "2023-10-05T08:43:33.825331513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Grid Search: loss*=66.92119404701603, w0*=72.72727272727272, w1*=10.606060606060595, execution time=0.019 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x600 with 3 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "from grid_search import grid_search_v2\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=100)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search_v2(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:56:39.062333459Z",
     "start_time": "2023-10-05T07:56:39.051103483Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    nb_sample = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    return -1/nb_sample * tx.T @ e\n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:56:40.092268461Z",
     "start_time": "2023-10-05T07:56:40.075270738Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        w = w - gamma*gradient\n",
    "        # ***************************************************\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:08:51.743360695Z",
     "start_time": "2023-10-05T08:08:51.696198134Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.2367127591674, w0=51.30574540147361, w1=9.43579870449228\n",
      "GD iter. 1/49: loss=265.3024621089606, w0=66.69746902191571, w1=12.266538315839998\n",
      "GD iter. 2/49: loss=37.87837955044126, w0=71.31498610804834, w1=13.115760199244328\n",
      "GD iter. 3/49: loss=17.41021212017447, w0=72.70024123388814, w1=13.37052676426563\n",
      "GD iter. 4/49: loss=15.568077051450452, w0=73.11581777164007, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=15.402284895265302, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.387363601208634, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.386020684743531, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638305, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.385887965652199, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.38588787754345, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.385887869613665, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.385887868899985, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.385887868835756, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829968, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.385887868829451, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.3858878688294, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.385887868829407, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829403, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.3858878688294, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.385887868829398, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.385887868829403, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829402, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.385887868829403, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.385887868829398, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.385887868829396, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.385887868829403, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.385887868829398, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.010 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:56:42.078582498Z",
     "start_time": "2023-10-05T07:56:41.665939340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "682ac0b475b94591989ee353396666dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.plot_figure(n_iter)>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "    \n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:23:31.760472499Z",
     "start_time": "2023-10-05T08:23:31.731669250Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from just few examples n and their corresponding y_n labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    nb_sample = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    return -1/nb_sample * tx.T @ e\n",
    "    # ***************************************************\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        gradient = np.zeros(tx.shape[1]) # init the gradient for the upcoming batch\n",
    "        for y_batch, tx_batch in batch_iter(y,tx, batch_size):\n",
    "            gradient += compute_stoch_gradient(y_batch, tx_batch, w)\n",
    "\n",
    "        w = w - gamma * gradient/batch_size\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # ***************************************************\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:24:30.766391420Z",
     "start_time": "2023-10-05T08:24:30.642208850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2792.2367127591674, w0=5.68321294116898, w1=-7.039907321989116\n",
      "SGD iter. 1/49: loss=2511.5172752155922, w0=14.225267620194348, w1=-1.2445104939420464\n",
      "SGD iter. 2/49: loss=1868.3402235440758, w0=22.228471410358175, w1=8.513831515048441\n",
      "SGD iter. 3/49: loss=1331.555996593422, w0=27.23331534324224, w1=8.10691154586556\n",
      "SGD iter. 4/49: loss=1090.6091254571572, w0=32.32941117889554, w1=18.667302289862995\n",
      "SGD iter. 5/49: loss=867.8870056124554, w0=36.236022769748715, w1=16.175189393581295\n",
      "SGD iter. 6/49: loss=705.6626336437237, w0=40.43322232513989, w1=11.414964289237563\n",
      "SGD iter. 7/49: loss=557.4302719513751, w0=43.238386258138036, w1=11.331147947689304\n",
      "SGD iter. 8/49: loss=469.36166697531553, w0=46.608148341794085, w1=9.92888716313055\n",
      "SGD iter. 9/49: loss=377.7553258491407, w0=48.543937082575475, w1=7.526406317061618\n",
      "SGD iter. 10/49: loss=339.3876914941795, w0=51.87464332996305, w1=6.404630919853675\n",
      "SGD iter. 11/49: loss=269.80702650917584, w0=52.917213158187714, w1=4.84717823250323\n",
      "SGD iter. 12/49: loss=260.25134290226526, w0=55.82575920603146, w1=4.708087869712989\n",
      "SGD iter. 13/49: loss=206.42494236097377, w0=57.31560530991382, w1=4.866246247232401\n",
      "SGD iter. 14/49: loss=180.13508991062344, w0=59.36494955620602, w1=7.049165906657306\n",
      "SGD iter. 15/49: loss=133.0699888946581, w0=62.642553630431514, w1=10.869851522859658\n",
      "SGD iter. 16/49: loss=75.51739895370497, w0=63.39508841586241, w1=10.911875831098227\n",
      "SGD iter. 17/49: loss=67.67623346503419, w0=64.74969998122336, w1=11.64316920169139\n",
      "SGD iter. 18/49: loss=53.574198363776105, w0=65.43257041321986, w1=11.75939275737592\n",
      "SGD iter. 19/49: loss=47.76606216745577, w0=65.83008593029274, w1=11.651531170438895\n",
      "SGD iter. 20/49: loss=44.91143569029984, w0=67.37010940215428, w1=11.449978511104929\n",
      "SGD iter. 21/49: loss=34.99157562938096, w0=67.79469103534228, w1=11.266551099051211\n",
      "SGD iter. 22/49: loss=32.95570003117652, w0=68.44315920324996, w1=11.535710106763531\n",
      "SGD iter. 23/49: loss=29.0404102602814, w0=68.37593633774148, w1=11.606510228205492\n",
      "SGD iter. 24/49: loss=29.233622620022267, w0=68.1248190451919, w1=11.70535326713369\n",
      "SGD iter. 25/49: loss=30.31987578669044, w0=68.90092219510211, w1=12.203487238721932\n",
      "SGD iter. 26/49: loss=25.849486896787454, w0=70.20938163138057, w1=12.686467657473225\n",
      "SGD iter. 27/49: loss=20.45770115667246, w0=71.13418052827662, w1=11.076032353691707\n",
      "SGD iter. 28/49: loss=20.60696845232981, w0=71.90662410661784, w1=11.39286721429898\n",
      "SGD iter. 29/49: loss=18.5256470817997, w0=72.48031916786763, w1=11.875062761897096\n",
      "SGD iter. 30/49: loss=17.004312941446145, w0=72.17190075492759, w1=11.755170602256316\n",
      "SGD iter. 31/49: loss=17.502375974810967, w0=72.62120209033658, w1=12.431902463323539\n",
      "SGD iter. 32/49: loss=16.161116777035225, w0=71.71513115565163, w1=12.874234059192393\n",
      "SGD iter. 33/49: loss=16.81548016903085, w0=71.21686393833478, w1=13.384810494725652\n",
      "SGD iter. 34/49: loss=17.54747615809911, w0=71.88449241044992, w1=13.902929661305087\n",
      "SGD iter. 35/49: loss=16.46869016607149, w0=72.0806068073534, w1=13.649089563672062\n",
      "SGD iter. 36/49: loss=16.136299055597632, w0=71.9089868193194, w1=13.759465080088733\n",
      "SGD iter. 37/49: loss=16.3840413703085, w0=72.06603448709589, w1=13.919527017950942\n",
      "SGD iter. 38/49: loss=16.23646017728023, w0=71.52199572621467, w1=12.837860041914976\n",
      "SGD iter. 39/49: loss=17.161736479672484, w0=72.82760555802501, w1=14.06182306042647\n",
      "SGD iter. 40/49: loss=15.664039771962768, w0=72.66884489562231, w1=13.969242485577166\n",
      "SGD iter. 41/49: loss=15.701068398568305, w0=72.54561417480963, w1=14.035203708817424\n",
      "SGD iter. 42/49: loss=15.820155448675038, w0=72.96363426401125, w1=13.421703282950078\n",
      "SGD iter. 43/49: loss=15.442115394657144, w0=72.91156568773759, w1=13.469515873922354\n",
      "SGD iter. 44/49: loss=15.459038029326585, w0=72.55891947383492, w1=13.21319560305184\n",
      "SGD iter. 45/49: loss=15.691517837964168, w0=72.50427433882146, w1=13.201626670035644\n",
      "SGD iter. 46/49: loss=15.736325431228988, w0=72.58184865773362, w1=13.101725961016964\n",
      "SGD iter. 47/49: loss=15.710848979964585, w0=72.95881714598505, w1=13.375672335643914\n",
      "SGD iter. 48/49: loss=15.447447672262921, w0=73.37233787328695, w1=13.748142855431874\n",
      "SGD iter. 49/49: loss=15.424989838565555, w0=73.376000208375, w1=13.752224351920578\n",
      "SGD: execution time=0.066 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:22:09.852037826Z",
     "start_time": "2023-10-05T08:22:09.485488673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35dbd505a2b54441b17d3c4b2edd7b12"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.plot_figure(n_iter)>"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:25:28.960981640Z",
     "start_time": "2023-10-05T08:25:28.903237746Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)\n",
    "# ***************************************************\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:09:39.920795334Z",
     "start_time": "2023-10-05T08:09:39.895305405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((200,), (200, 2))"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:10:37.801456654Z",
     "start_time": "2023-10-05T08:10:37.781489408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.835114535854, w0=51.84746409844846, w1=7.724426406192428\n",
      "GD iter. 1/49: loss=318.2821247015954, w0=67.40170332798299, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165127, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.9747763988552, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631794\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.0516072257859, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248088, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140234, w0=74.06776649225755, w1=11.03488900159354\n",
      "GD iter. 12/49: loss=65.93073010339528, w0=74.06779404612573, w1=11.034893106670433\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260343, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873673\n",
      "GD iter. 21/49: loss=65.93073010260339, w0=74.06780585469393, w1=11.034894865954472\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260338, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988166\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988822\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989093\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T07:56:53.270899444Z",
     "start_time": "2023-10-05T07:56:52.885292788Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94050fda60934180afaf3a7deda6596f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.plot_figure(n_iter)>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:00:13.520979743Z",
     "start_time": "2023-10-05T08:00:13.503344344Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "\n",
    "    #return -1/nb_sample * tx.T @ e\n",
    "    nb_sample = y.shape[0]\n",
    "    e = y - tx @ w\n",
    "    return -1/nb_sample * tx.T @ np.sign(e)\n",
    "    \n",
    "    # ***************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:06:25.653984286Z",
     "start_time": "2023-10-05T08:06:25.591651835Z"
    }
   },
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        loss = compute_loss_MAE(y, tx, w)\n",
    "        subgradient = compute_subgradient_mae(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        w = w - gamma*subgradient\n",
    "        # ***************************************************\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:10:46.384137784Z",
     "start_time": "2023-10-05T08:10:46.349344251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7000000000000004, w1=7.625844400394043e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4000000000000008, w1=1.5251688800788087e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.1000000000000014, w1=2.287753320118213e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492637, w0=2.8000000000000016, w1=3.0503377601576174e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5000000000000018, w1=3.812922200197022e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492639, w0=4.200000000000002, w1=4.575506640236426e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.900000000000002, w1=5.3380910802758305e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492637, w0=5.600000000000002, w1=6.100675520315235e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492637, w0=6.3000000000000025, w1=6.863259960354639e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000003, w1=7.625844400394044e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000003, w1=8.388428840433449e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.400000000000004, w1=9.151013280472854e-15\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.100000000000005, w1=9.913597720512259e-15\n",
      "SubGD iter. 13/499: loss=64.96780585492637, w0=9.800000000000006, w1=1.0676182160551664e-14\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.500000000000007, w1=1.1438766600591069e-14\n",
      "SubGD iter. 15/499: loss=63.56780585492637, w0=11.200000000000008, w1=1.2201351040630474e-14\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.90000000000001, w1=1.2963935480669879e-14\n",
      "SubGD iter. 17/499: loss=62.16780585492637, w0=12.60000000000001, w1=1.3726519920709284e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492636, w0=13.300000000000011, w1=1.448910436074869e-14\n",
      "SubGD iter. 19/499: loss=60.76780585492637, w0=14.000000000000012, w1=1.5251688800788094e-14\n",
      "SubGD iter. 20/499: loss=60.06780585492637, w0=14.700000000000014, w1=1.60142732408275e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492637, w0=15.400000000000015, w1=1.6776857680866904e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926364, w0=16.100000000000016, w1=1.753944212090631e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492636, w0=16.800000000000015, w1=1.8302026560945714e-14\n",
      "SubGD iter. 24/499: loss=57.267805854926365, w0=17.500000000000014, w1=1.906461100098512e-14\n",
      "SubGD iter. 25/499: loss=56.56780585492637, w0=18.200000000000014, w1=1.9827195441024524e-14\n",
      "SubGD iter. 26/499: loss=55.86780585492637, w0=18.900000000000013, w1=2.058977988106393e-14\n",
      "SubGD iter. 27/499: loss=55.16780585492637, w0=19.600000000000012, w1=2.1352364321103335e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492636, w0=20.30000000000001, w1=2.211494876114274e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926365, w0=21.00000000000001, w1=2.2877533201182145e-14\n",
      "SubGD iter. 30/499: loss=53.06780585492637, w0=21.70000000000001, w1=2.364011764122155e-14\n",
      "SubGD iter. 31/499: loss=52.36780585492637, w0=22.40000000000001, w1=2.4402702081260955e-14\n",
      "SubGD iter. 32/499: loss=51.66780585492637, w0=23.10000000000001, w1=2.516528652130036e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492636, w0=23.800000000000008, w1=2.5927870961339765e-14\n",
      "SubGD iter. 34/499: loss=50.26780585492637, w0=24.500000000000007, w1=2.669045540137917e-14\n",
      "SubGD iter. 35/499: loss=49.56780585492638, w0=25.200000000000006, w1=2.7453039841418575e-14\n",
      "SubGD iter. 36/499: loss=48.86780585492637, w0=25.900000000000006, w1=2.821562428145798e-14\n",
      "SubGD iter. 37/499: loss=48.16780585492637, w0=26.600000000000005, w1=2.8978208721497385e-14\n",
      "SubGD iter. 38/499: loss=47.46780585492637, w0=27.300000000000004, w1=2.9740793161536787e-14\n",
      "SubGD iter. 39/499: loss=46.76780585492637, w0=28.000000000000004, w1=3.050337760157619e-14\n",
      "SubGD iter. 40/499: loss=46.06780585492638, w0=28.700000000000003, w1=3.126596204161559e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926374, w0=29.400000000000002, w1=3.202854648165499e-14\n",
      "SubGD iter. 42/499: loss=44.66780585492637, w0=30.1, w1=3.2791130921694394e-14\n",
      "SubGD iter. 43/499: loss=43.96780585492637, w0=30.8, w1=3.3553715361733796e-14\n",
      "SubGD iter. 44/499: loss=43.26780585492637, w0=31.5, w1=3.43162998017732e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926384, w0=32.2, w1=3.50788842418126e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926374, w0=32.900000000000006, w1=3.5841468681852e-14\n",
      "SubGD iter. 47/499: loss=41.16780585492638, w0=33.60000000000001, w1=3.6604053121891404e-14\n",
      "SubGD iter. 48/499: loss=40.46780585492637, w0=34.30000000000001, w1=3.7366637561930805e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926365, w0=35.000000000000014, w1=3.812922200197021e-14\n",
      "SubGD iter. 50/499: loss=39.06780585492637, w0=35.70000000000002, w1=3.889180644200961e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492637, w0=36.40000000000002, w1=3.965439088204901e-14\n",
      "SubGD iter. 52/499: loss=37.667805854926364, w0=37.10000000000002, w1=4.041697532208841e-14\n",
      "SubGD iter. 53/499: loss=36.967805854926354, w0=37.800000000000026, w1=4.1179559762127815e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492636, w0=38.50000000000003, w1=4.1942144202167217e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492635, w0=39.20000000000003, w1=4.270472864220662e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492634, w0=39.900000000000034, w1=4.346731308224602e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492634, w0=40.60000000000004, w1=4.422989752228542e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492633, w0=41.30000000000004, w1=4.4992481962324824e-14\n",
      "SubGD iter. 59/499: loss=32.76780585492634, w0=42.00000000000004, w1=4.5755066402364226e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926334, w0=42.700000000000045, w1=4.651765084240363e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492633, w0=43.40000000000005, w1=4.728023528244303e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926332, w0=44.10000000000005, w1=4.804281972248243e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926326, w0=44.800000000000054, w1=4.8805404162521834e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926323, w0=45.50000000000006, w1=4.9567988602561235e-14\n",
      "SubGD iter. 65/499: loss=28.56780585492632, w0=46.20000000000006, w1=5.033057304260064e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926317, w0=46.90000000000006, w1=5.109315748264004e-14\n",
      "SubGD iter. 67/499: loss=27.173270209668893, w0=47.59306930693076, w1=0.011147845678281268\n",
      "SubGD iter. 68/499: loss=26.490451563751183, w0=48.279207920792146, w1=0.03308574108990965\n",
      "SubGD iter. 69/499: loss=25.817212322770157, w0=48.965346534653534, w1=0.055023636501538034\n",
      "SubGD iter. 70/499: loss=25.155039434656434, w0=49.630693069307, w1=0.10538326388308852\n",
      "SubGD iter. 71/499: loss=24.524103413894757, w0=50.28910891089116, w1=0.16746568532794484\n",
      "SubGD iter. 72/499: loss=23.89929534603557, w0=50.94752475247532, w1=0.22954810677280116\n",
      "SubGD iter. 73/499: loss=23.284392925657123, w0=51.59207920792086, w1=0.31242512932748595\n",
      "SubGD iter. 74/499: loss=22.686876444181824, w0=52.22277227722779, w1=0.4119501328840099\n",
      "SubGD iter. 75/499: loss=22.10626756964053, w0=52.84653465346541, w1=0.5208167847923866\n",
      "SubGD iter. 76/499: loss=21.53781882800841, w0=53.45643564356442, w1=0.6457900912636104\n",
      "SubGD iter. 77/499: loss=20.986339874628445, w0=54.059405940594125, w1=0.7796904498577328\n",
      "SubGD iter. 78/499: loss=20.44556093662042, w0=54.655445544554524, w1=0.9197570104995809\n",
      "SubGD iter. 79/499: loss=19.91191015895782, w0=55.244554455445616, w1=1.0670920297850033\n",
      "SubGD iter. 80/499: loss=19.389644090563205, w0=55.81980198019809, w1=1.2261255948210887\n",
      "SubGD iter. 81/499: loss=18.88798906439586, w0=56.36732673267334, w1=1.4107093426222252\n",
      "SubGD iter. 82/499: loss=18.41596050185421, w0=56.900990099009974, w1=1.6058537322202813\n",
      "SubGD iter. 83/499: loss=17.95489854304036, w0=57.4277227722773, w1=1.8087628022939741\n",
      "SubGD iter. 84/499: loss=17.505757656579803, w0=57.9336633663367, w1=2.0285064197514817\n",
      "SubGD iter. 85/499: loss=17.074957426931594, w0=58.4326732673268, w1=2.24943708486729\n",
      "SubGD iter. 86/499: loss=16.65296729750988, w0=58.91089108910898, w1=2.4837982986028466\n",
      "SubGD iter. 87/499: loss=16.248540731496703, w0=59.38217821782185, w1=2.7260245553531632\n",
      "SubGD iter. 88/499: loss=15.849105212654136, w0=59.83960396039611, w1=2.9787423334691487\n",
      "SubGD iter. 89/499: loss=15.466919791231307, w0=60.26237623762383, w1=3.251528669355451\n",
      "SubGD iter. 90/499: loss=15.108294621512195, w0=60.67821782178225, w1=3.5270865794242927\n",
      "SubGD iter. 91/499: loss=14.754896345922813, w0=61.087128712871355, w1=3.8064591839518287\n",
      "SubGD iter. 92/499: loss=14.404528961620256, w0=61.49603960396046, w1=4.085831788479364\n",
      "SubGD iter. 93/499: loss=14.055787028127256, w0=61.891089108910954, w1=4.373839384328622\n",
      "SubGD iter. 94/499: loss=13.714620911605614, w0=62.27920792079214, w1=4.666037469532062\n",
      "SubGD iter. 95/499: loss=13.381236307284132, w0=62.653465346534716, w1=4.959829093241784\n",
      "SubGD iter. 96/499: loss=13.058821615166217, w0=63.020792079207986, w1=5.257057192056655\n",
      "SubGD iter. 97/499: loss=12.740251724339217, w0=63.38118811881195, w1=5.560434316352422\n",
      "SubGD iter. 98/499: loss=12.42321888875609, w0=63.74158415841591, w1=5.86381144064819\n",
      "SubGD iter. 99/499: loss=12.10756173190115, w0=64.08811881188126, w1=6.172402175278565\n",
      "SubGD iter. 100/499: loss=11.800622097398117, w0=64.42772277227729, w1=6.486369310516515\n",
      "SubGD iter. 101/499: loss=11.495041794646406, w0=64.76732673267333, w1=6.800336445754466\n",
      "SubGD iter. 102/499: loss=11.189461491894695, w0=65.10693069306936, w1=7.114303580992416\n",
      "SubGD iter. 103/499: loss=10.883881189142983, w0=65.44653465346539, w1=7.428270716230367\n",
      "SubGD iter. 104/499: loss=10.584593408313182, w0=65.76534653465352, w1=7.747893210218644\n",
      "SubGD iter. 105/499: loss=10.295816534318924, w0=66.07029702970303, w1=8.073669686866923\n",
      "SubGD iter. 106/499: loss=10.011352081221341, w0=66.37524752475254, w1=8.399446163515202\n",
      "SubGD iter. 107/499: loss=9.72808432666811, w0=66.66633663366343, w1=8.732970280417408\n",
      "SubGD iter. 108/499: loss=9.448125461122489, w0=66.95742574257433, w1=9.066494397319614\n",
      "SubGD iter. 109/499: loss=9.171041104096652, w0=67.23465346534661, w1=9.398630319470307\n",
      "SubGD iter. 110/499: loss=8.903656131158945, w0=67.51188118811889, w1=9.730766241621\n",
      "SubGD iter. 111/499: loss=8.636271158221236, w0=67.78910891089117, w1=10.062902163771692\n",
      "SubGD iter. 112/499: loss=8.376151920302355, w0=68.06633663366345, w1=10.36399928997944\n",
      "SubGD iter. 113/499: loss=8.140540838751479, w0=68.32970297029712, w1=10.66046690927363\n",
      "SubGD iter. 114/499: loss=7.918544501597256, w0=68.59306930693079, w1=10.943174379960832\n",
      "SubGD iter. 115/499: loss=7.705279728376982, w0=68.85643564356445, w1=11.225881850648033\n",
      "SubGD iter. 116/499: loss=7.4936958311786235, w0=69.11287128712881, w1=11.504395843582225\n",
      "SubGD iter. 117/499: loss=7.289992405743398, w0=69.35544554455456, w1=11.78820189306777\n",
      "SubGD iter. 118/499: loss=7.0972340357815265, w0=69.58415841584169, w1=12.06091146519099\n",
      "SubGD iter. 119/499: loss=6.919905294668907, w0=69.8059405940595, w1=12.324245668386068\n",
      "SubGD iter. 120/499: loss=6.750573527315438, w0=70.02772277227733, w1=12.587579871581145\n",
      "SubGD iter. 121/499: loss=6.584744810805648, w0=70.25643564356446, w1=12.824765405096503\n",
      "SubGD iter. 122/499: loss=6.43034327634779, w0=70.47821782178228, w1=13.065616959310168\n",
      "SubGD iter. 123/499: loss=6.278071481890337, w0=70.6930693069308, w1=13.302953389983932\n",
      "SubGD iter. 124/499: loss=6.1336633292633085, w0=70.8940594059407, w1=13.525403099312937\n",
      "SubGD iter. 125/499: loss=6.005840798343017, w0=71.08811881188129, w1=13.742945617944232\n",
      "SubGD iter. 126/499: loss=5.885021825223205, w0=71.27524752475257, w1=13.953548196006865\n",
      "SubGD iter. 127/499: loss=5.771635252269645, w0=71.46237623762386, w1=14.1641507740695\n",
      "SubGD iter. 128/499: loss=5.667162061790244, w0=71.62178217821791, w1=14.349779559473198\n",
      "SubGD iter. 129/499: loss=5.586726765993134, w0=71.75346534653474, w1=14.516890107612335\n",
      "SubGD iter. 130/499: loss=5.523847812160379, w0=71.87128712871295, w1=14.67079118532421\n",
      "SubGD iter. 131/499: loss=5.4800937085918635, w0=71.95445544554464, w1=14.780276456654546\n",
      "SubGD iter. 132/499: loss=5.4530880035020175, w0=72.03762376237633, w1=14.889761727984881\n",
      "SubGD iter. 133/499: loss=5.427392630862901, w0=72.1069306930694, w1=14.985916181776751\n",
      "SubGD iter. 134/499: loss=5.407322445682746, w0=72.17623762376247, w1=15.082070635568622\n",
      "SubGD iter. 135/499: loss=5.387252260502593, w0=72.24554455445555, w1=15.178225089360492\n",
      "SubGD iter. 136/499: loss=5.370460780338691, w0=72.30099009901001, w1=15.259723489715935\n",
      "SubGD iter. 137/499: loss=5.3574065233347365, w0=72.34950495049516, w1=15.335091856448162\n",
      "SubGD iter. 138/499: loss=5.345929264022579, w0=72.39801980198031, w1=15.41046022318039\n",
      "SubGD iter. 139/499: loss=5.33571465951747, w0=72.43267326732685, w1=15.46996178675575\n",
      "SubGD iter. 140/499: loss=5.330043910465358, w0=72.46039603960408, w1=15.518645285832834\n",
      "SubGD iter. 141/499: loss=5.325676428273224, w0=72.48811881188131, w1=15.561592159086512\n",
      "SubGD iter. 142/499: loss=5.322176726526589, w0=72.50198019801992, w1=15.59782833203255\n",
      "SubGD iter. 143/499: loss=5.32011130964311, w0=72.52277227722784, w1=15.624722856626738\n",
      "SubGD iter. 144/499: loss=5.318478284898437, w0=72.55049504950507, w1=15.642690329098025\n",
      "SubGD iter. 145/499: loss=5.317240048565144, w0=72.56435643564369, w1=15.664356578291116\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485161, w1=15.677095775361309\n",
      "SubGD iter. 147/499: loss=5.315557122666142, w0=72.60594059405953, w1=15.689834972431502\n",
      "SubGD iter. 148/499: loss=5.314707697380738, w0=72.62673267326745, w1=15.702574169501695\n",
      "SubGD iter. 149/499: loss=5.313876880922166, w0=72.64059405940607, w1=15.724240418694786\n",
      "SubGD iter. 150/499: loss=5.313052246871382, w0=72.66138613861399, w1=15.736979615764978\n",
      "SubGD iter. 151/499: loss=5.312377839024388, w0=72.6683168316833, w1=15.748110294231305\n",
      "SubGD iter. 152/499: loss=5.312132229725042, w0=72.6752475247526, w1=15.759240972697631\n",
      "SubGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782191, w1=15.770371651163957\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782191, w1=15.774323911906711\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782191, w1=15.778276172649464\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782191, w1=15.782228433392218\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782191, w1=15.786180694134972\n",
      "SubGD iter. 158/499: loss=5.311594306869985, w0=72.68217821782191, w1=15.790132954877725\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782191, w1=15.794085215620479\n",
      "SubGD iter. 160/499: loss=5.311549677255759, w0=72.68217821782191, w1=15.798037476363232\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782191, w1=15.801989737105986\n",
      "SubGD iter. 162/499: loss=5.3115050476415355, w0=72.68217821782191, w1=15.80594199784874\n",
      "SubGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782191, w1=15.809894258591493\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782191, w1=15.813846519334247\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782191, w1=15.817798780077\n",
      "SubGD iter. 166/499: loss=5.311415788413084, w0=72.68217821782191, w1=15.821751040819754\n",
      "SubGD iter. 167/499: loss=5.311393473605972, w0=72.68217821782191, w1=15.825703301562507\n",
      "SubGD iter. 168/499: loss=5.311371158798861, w0=72.68217821782191, w1=15.82965556230526\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782191, w1=15.833607823048014\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782191, w1=15.837560083790768\n",
      "SubGD iter. 171/499: loss=5.3113042143775235, w0=72.68217821782191, w1=15.841512344533522\n",
      "SubGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782191, w1=15.845464605276275\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782191, w1=15.849416866019029\n",
      "SubGD iter. 174/499: loss=5.311237269956186, w0=72.68217821782191, w1=15.853369126761782\n",
      "SubGD iter. 175/499: loss=5.3112149551490715, w0=72.68217821782191, w1=15.857321387504536\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782191, w1=15.86127364824729\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782191, w1=15.865225908990043\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782191, w1=15.869178169732796\n",
      "SubGD iter. 179/499: loss=5.311125695920623, w0=72.68217821782191, w1=15.87313043047555\n",
      "SubGD iter. 180/499: loss=5.3111033811135115, w0=72.68217821782191, w1=15.877082691218304\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782191, w1=15.881034951961057\n",
      "SubGD iter. 182/499: loss=5.311058751499286, w0=72.68217821782191, w1=15.88498721270381\n",
      "SubGD iter. 183/499: loss=5.311036436692174, w0=72.68217821782191, w1=15.888939473446564\n",
      "SubGD iter. 184/499: loss=5.31101412188506, w0=72.68217821782191, w1=15.892891734189318\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782191, w1=15.896843994932071\n",
      "SubGD iter. 186/499: loss=5.310969492270836, w0=72.68217821782191, w1=15.900796255674825\n",
      "SubGD iter. 187/499: loss=5.3109471774637225, w0=72.68217821782191, w1=15.904748516417579\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782191, w1=15.908700777160332\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782191, w1=15.912653037903086\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.6752475247526, w1=15.910526938117348\n",
      "SubGD iter. 191/499: loss=5.31089223718627, w0=72.6752475247526, w1=15.914479198860102\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.6752475247526, w1=15.918431459602855\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.6683168316833, w1=15.916305359817118\n",
      "SubGD iter. 194/499: loss=5.310859611715927, w0=72.6683168316833, w1=15.920257620559871\n",
      "SubGD iter. 195/499: loss=5.310837296908815, w0=72.6683168316833, w1=15.924209881302625\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.6683168316833, w1=15.928162142045379\n",
      "SubGD iter. 197/499: loss=5.310823570190171, w0=72.66138613861399, w1=15.926036042259641\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861399, w1=15.929988303002395\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861399, w1=15.933940563745148\n",
      "SubGD iter. 200/499: loss=5.310772500182622, w0=72.65445544554468, w1=15.93181446395941\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554468, w1=15.935766724702164\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554468, w1=15.939718985444918\n",
      "SubGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554468, w1=15.943671246187671\n",
      "SubGD iter. 204/499: loss=5.31073343431896, w0=72.64752475247538, w1=15.941545146401934\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247538, w1=15.945497407144687\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247538, w1=15.949449667887441\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940607, w1=15.947323568101703\n",
      "SubGD iter. 208/499: loss=5.3106844802203375, w0=72.64059405940607, w1=15.951275828844457\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940607, w1=15.95522808958721\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940607, w1=15.959180350329964\n",
      "SubGD iter. 211/499: loss=5.310643298447748, w0=72.63366336633676, w1=15.957054250544227\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633676, w1=15.96100651128698\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633676, w1=15.964958772029734\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326745, w1=15.962832672243996\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633676, w1=15.9673013720514\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326745, w1=15.965175272265663\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633676, w1=15.969643972073067\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326745, w1=15.96751787228733\n",
      "SubGD iter. 219/499: loss=5.310603273452553, w0=72.63366336633676, w1=15.971986572094734\n",
      "SubGD iter. 220/499: loss=5.31061357387479, w0=72.62673267326745, w1=15.969860472308996\n",
      "SubGD iter. 221/499: loss=5.310588318629315, w0=72.63366336633676, w1=15.9743291721164\n",
      "SubGD iter. 222/499: loss=5.310620689019653, w0=72.62673267326745, w1=15.972203072330663\n",
      "SubGD iter. 223/499: loss=5.3105749661498844, w0=72.62673267326745, w1=15.970593411609576\n",
      "SubGD iter. 224/499: loss=5.310583639649727, w0=72.63366336633676, w1=15.97506211141698\n",
      "SubGD iter. 225/499: loss=5.310622915165494, w0=72.62673267326745, w1=15.972936011631242\n",
      "SubGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326745, w1=15.971326350910156\n",
      "SubGD iter. 227/499: loss=5.31057896067014, w0=72.63366336633676, w1=15.97579505071756\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326745, w1=15.973668950931822\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326745, w1=15.972059290210735\n",
      "SubGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326745, w1=15.970449629489648\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633676, w1=15.974918329297052\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326745, w1=15.972792229511315\n",
      "SubGD iter. 233/499: loss=5.310576320925845, w0=72.62673267326745, w1=15.971182568790228\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633676, w1=15.975651268597632\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326745, w1=15.973525168811895\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326745, w1=15.971915508090808\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633676, w1=15.976384207898212\n",
      "SubGD iter. 238/499: loss=5.310626930749845, w0=72.62673267326745, w1=15.974258108112474\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326745, w1=15.972648447391387\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326745, w1=15.9710387866703\n",
      "SubGD iter. 241/499: loss=5.310580796439088, w0=72.63366336633676, w1=15.975507486477705\n",
      "SubGD iter. 242/499: loss=5.310624267896666, w0=72.62673267326745, w1=15.973381386691967\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326745, w1=15.97177172597088\n",
      "SubGD iter. 244/499: loss=5.3105761174595, w0=72.63366336633676, w1=15.976240425778284\n",
      "SubGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326745, w1=15.974114325992547\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326745, w1=15.97250466527146\n",
      "SubGD iter. 247/499: loss=5.310575659667473, w0=72.62673267326745, w1=15.970895004550373\n",
      "SubGD iter. 248/499: loss=5.310581714323562, w0=72.63366336633676, w1=15.975363704357777\n",
      "SubGD iter. 249/499: loss=5.310623831189333, w0=72.62673267326745, w1=15.97323760457204\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326745, w1=15.971627943850953\n",
      "SubGD iter. 251/499: loss=5.310577035343974, w0=72.63366336633676, w1=15.976096643658357\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326745, w1=15.97397054387262\n",
      "SubGD iter. 253/499: loss=5.310579030477768, w0=72.62673267326745, w1=15.972360883151532\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326745, w1=15.970751222430446\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633676, w1=15.97521992223785\n",
      "SubGD iter. 256/499: loss=5.310623394481998, w0=72.62673267326745, w1=15.973093822452112\n",
      "SubGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326745, w1=15.971484161731025\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633676, w1=15.97595286153843\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326745, w1=15.973826761752692\n",
      "SubGD iter. 260/499: loss=5.310578699848579, w0=72.62673267326745, w1=15.972217101031605\n",
      "SubGD iter. 261/499: loss=5.310574998409099, w0=72.62673267326745, w1=15.970607440310518\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633676, w1=15.975076140117922\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326745, w1=15.972950040332185\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326745, w1=15.971340379611098\n",
      "SubGD iter. 265/499: loss=5.310578871112921, w0=72.63366336633676, w1=15.975809079418502\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326745, w1=15.973682979632764\n",
      "SubGD iter. 267/499: loss=5.310578369219392, w0=72.62673267326745, w1=15.972073318911677\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326745, w1=15.97046365819059\n",
      "SubGD iter. 269/499: loss=5.310584467976983, w0=72.63366336633676, w1=15.974932357997995\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326745, w1=15.972806258212257\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326745, w1=15.97119659749117\n",
      "SubGD iter. 272/499: loss=5.310579788997397, w0=72.63366336633676, w1=15.975665297298574\n",
      "SubGD iter. 273/499: loss=5.310624747213173, w0=72.62673267326745, w1=15.973539197512837\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326745, w1=15.97192953679175\n",
      "SubGD iter. 275/499: loss=5.310575110017809, w0=72.63366336633676, w1=15.976398236599154\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326745, w1=15.974272136813417\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326745, w1=15.97266247609233\n",
      "SubGD iter. 278/499: loss=5.310576022555871, w0=72.62673267326745, w1=15.971052815371243\n",
      "SubGD iter. 279/499: loss=5.310580706881869, w0=72.63366336633676, w1=15.975521515178647\n",
      "SubGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326745, w1=15.97339541539291\n",
      "SubGD iter. 281/499: loss=5.310577707961018, w0=72.62673267326745, w1=15.971785754671822\n",
      "SubGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633676, w1=15.976254454479227\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326745, w1=15.97412835469349\n",
      "SubGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326745, w1=15.972518693972402\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326745, w1=15.970909033251315\n",
      "SubGD iter. 286/499: loss=5.310581624766343, w0=72.63366336633676, w1=15.97537773305872\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326745, w1=15.973251633272982\n",
      "SubGD iter. 288/499: loss=5.310577377331833, w0=72.62673267326745, w1=15.971641972551895\n",
      "SubGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633676, w1=15.9761106723593\n",
      "SubGD iter. 290/499: loss=5.310626099944345, w0=72.62673267326745, w1=15.973984572573562\n",
      "SubGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326745, w1=15.972374911852475\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326745, w1=15.970765251131388\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633676, w1=15.975233950938792\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326745, w1=15.973107851153054\n",
      "SubGD iter. 295/499: loss=5.310577046702646, w0=72.62673267326745, w1=15.971498190431968\n",
      "SubGD iter. 296/499: loss=5.310577863671229, w0=72.63366336633676, w1=15.975966890239372\n",
      "SubGD iter. 297/499: loss=5.310625663237011, w0=72.62673267326745, w1=15.973840790453634\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326745, w1=15.972231129732547\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326745, w1=15.97062146901146\n",
      "SubGD iter. 300/499: loss=5.310583460535291, w0=72.63366336633676, w1=15.975090168818864\n",
      "SubGD iter. 301/499: loss=5.310623000383833, w0=72.62673267326745, w1=15.972964069033127\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326745, w1=15.97135440831204\n",
      "SubGD iter. 303/499: loss=5.310578781555704, w0=72.63366336633676, w1=15.975823108119444\n",
      "SubGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326745, w1=15.973697008333707\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326745, w1=15.97208734761262\n",
      "SubGD iter. 306/499: loss=5.3105747000391235, w0=72.62673267326745, w1=15.970477686891533\n",
      "SubGD iter. 307/499: loss=5.3105843784197635, w0=72.63366336633676, w1=15.974946386698937\n",
      "SubGD iter. 308/499: loss=5.310622563676499, w0=72.62673267326745, w1=15.9728202869132\n",
      "SubGD iter. 309/499: loss=5.310576385444271, w0=72.62673267326745, w1=15.971210626192113\n",
      "SubGD iter. 310/499: loss=5.310579699440177, w0=72.63366336633676, w1=15.975679325999517\n",
      "SubGD iter. 311/499: loss=5.310624789822341, w0=72.62673267326745, w1=15.97355322621378\n",
      "SubGD iter. 312/499: loss=5.3105780708494175, w0=72.62673267326745, w1=15.971943565492692\n",
      "SubGD iter. 313/499: loss=5.310575020460588, w0=72.63366336633676, w1=15.976412265300096\n",
      "SubGD iter. 314/499: loss=5.310627015968183, w0=72.62673267326745, w1=15.974286165514359\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326745, w1=15.972676504793272\n",
      "SubGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326745, w1=15.971066844072185\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633676, w1=15.97553554387959\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326745, w1=15.973409444093852\n",
      "SubGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326745, w1=15.971799783372765\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633676, w1=15.976268483180169\n",
      "SubGD iter. 321/499: loss=5.310626579260849, w0=72.62673267326745, w1=15.974142383394431\n",
      "SubGD iter. 322/499: loss=5.310579425625379, w0=72.62673267326745, w1=15.972532722673344\n",
      "SubGD iter. 323/499: loss=5.310575724185898, w0=72.62673267326745, w1=15.970923061952258\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633676, w1=15.975391761759662\n",
      "SubGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326745, w1=15.973265661973924\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326745, w1=15.971656001252837\n",
      "SubGD iter. 327/499: loss=5.310576856229537, w0=72.63366336633676, w1=15.976124701060241\n",
      "SubGD iter. 328/499: loss=5.310626142553515, w0=72.62673267326745, w1=15.973998601274504\n",
      "SubGD iter. 329/499: loss=5.310579094996193, w0=72.62673267326745, w1=15.972388940553417\n",
      "SubGD iter. 330/499: loss=5.310575393556711, w0=72.62673267326745, w1=15.97077927983233\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633676, w1=15.975247979639734\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326745, w1=15.973121879853997\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326745, w1=15.97151221913291\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633676, w1=15.975980918940314\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326745, w1=15.973854819154576\n",
      "SubGD iter. 336/499: loss=5.310578764367006, w0=72.62673267326745, w1=15.97224515843349\n",
      "SubGD iter. 337/499: loss=5.310575062927523, w0=72.62673267326745, w1=15.970635497712403\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633676, w1=15.975104197519807\n",
      "SubGD iter. 339/499: loss=5.310623042993001, w0=72.62673267326745, w1=15.97297809773407\n",
      "SubGD iter. 340/499: loss=5.31057674833267, w0=72.62673267326745, w1=15.971368437012982\n",
      "SubGD iter. 341/499: loss=5.310578691998485, w0=72.63366336633676, w1=15.975837136820386\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326745, w1=15.973711037034649\n",
      "SubGD iter. 343/499: loss=5.310578433737818, w0=72.62673267326745, w1=15.972101376313562\n",
      "SubGD iter. 344/499: loss=5.310574732298337, w0=72.62673267326745, w1=15.970491715592475\n",
      "SubGD iter. 345/499: loss=5.310584288862548, w0=72.63366336633676, w1=15.97496041539988\n",
      "SubGD iter. 346/499: loss=5.310622606285666, w0=72.62673267326745, w1=15.972834315614142\n",
      "SubGD iter. 347/499: loss=5.3105764177034835, w0=72.62673267326745, w1=15.971224654893055\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633676, w1=15.975693354700459\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326745, w1=15.973567254914721\n",
      "SubGD iter. 350/499: loss=5.310578103108631, w0=72.62673267326745, w1=15.971957594193634\n",
      "SubGD iter. 351/499: loss=5.310574930903369, w0=72.63366336633676, w1=15.976426294001039\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326745, w1=15.974300194215301\n",
      "SubGD iter. 353/499: loss=5.310579788513779, w0=72.62673267326745, w1=15.972690533494214\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326745, w1=15.971080872773127\n",
      "SubGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633676, w1=15.975549572580531\n",
      "SubGD iter. 356/499: loss=5.310624395724174, w0=72.62673267326745, w1=15.973423472794794\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326745, w1=15.971813812073707\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633676, w1=15.976282511881111\n",
      "SubGD iter. 359/499: loss=5.310626621870017, w0=72.62673267326745, w1=15.974156412095374\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326745, w1=15.972546751374287\n",
      "SubGD iter. 361/499: loss=5.310575756445111, w0=72.62673267326745, w1=15.9709370906532\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633676, w1=15.975405790460604\n",
      "SubGD iter. 363/499: loss=5.310623959016838, w0=72.62673267326745, w1=15.973279690674866\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326745, w1=15.97167002995378\n",
      "SubGD iter. 365/499: loss=5.310576766672318, w0=72.63366336633676, w1=15.976138729761184\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326745, w1=15.974012629975446\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326745, w1=15.97240296925436\n",
      "SubGD iter. 368/499: loss=5.310575425815922, w0=72.62673267326745, w1=15.970793308533272\n",
      "SubGD iter. 369/499: loss=5.310582363536379, w0=72.63366336633676, w1=15.975262008340676\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326745, w1=15.973135908554939\n",
      "SubGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326745, w1=15.971526247833852\n",
      "SubGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633676, w1=15.975994947641256\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326745, w1=15.973868847855519\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326745, w1=15.972259187134432\n",
      "SubGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326745, w1=15.970649526413345\n",
      "SubGD iter. 376/499: loss=5.310583281420853, w0=72.63366336633676, w1=15.975118226220749\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326745, w1=15.972992126435011\n",
      "SubGD iter. 378/499: loss=5.310576780591883, w0=72.62673267326745, w1=15.971382465713925\n",
      "SubGD iter. 379/499: loss=5.310578602441265, w0=72.63366336633676, w1=15.975851165521329\n",
      "SubGD iter. 380/499: loss=5.310625311748013, w0=72.62673267326745, w1=15.973725065735591\n",
      "SubGD iter. 381/499: loss=5.310578465997032, w0=72.62673267326745, w1=15.972115405014504\n",
      "SubGD iter. 382/499: loss=5.310574764557549, w0=72.62673267326745, w1=15.970505744293417\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633676, w1=15.974974444100821\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326745, w1=15.972848344315084\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326745, w1=15.971238683593997\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633676, w1=15.975707383401401\n",
      "SubGD iter. 387/499: loss=5.310624875040678, w0=72.62673267326745, w1=15.973581283615664\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326745, w1=15.971971622894577\n",
      "SubGD iter. 389/499: loss=5.310574841346151, w0=72.63366336633676, w1=15.976440322701981\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326745, w1=15.974314222916243\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326745, w1=15.972704562195156\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326745, w1=15.97109490147407\n",
      "SubGD iter. 393/499: loss=5.310580438210213, w0=72.63366336633676, w1=15.975563601281474\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326745, w1=15.973437501495736\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326745, w1=15.97182784077465\n",
      "SubGD iter. 396/499: loss=5.310575759230625, w0=72.63366336633676, w1=15.976296540582053\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326745, w1=15.974170440796316\n",
      "SubGD iter. 398/499: loss=5.310579490143805, w0=72.62673267326745, w1=15.972560780075229\n",
      "SubGD iter. 399/499: loss=5.310575788704322, w0=72.62673267326745, w1=15.970951119354142\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633676, w1=15.975419819161546\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326745, w1=15.973293719375809\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326745, w1=15.971684058654722\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633676, w1=15.976152758462126\n",
      "SubGD iter. 404/499: loss=5.310626227771851, w0=72.62673267326745, w1=15.974026658676388\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326745, w1=15.972416997955301\n",
      "SubGD iter. 406/499: loss=5.310575458075135, w0=72.62673267326745, w1=15.970807337234215\n",
      "SubGD iter. 407/499: loss=5.31058227397916, w0=72.63366336633676, w1=15.975276037041619\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326745, w1=15.973149937255881\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326745, w1=15.971540276534794\n",
      "SubGD iter. 410/499: loss=5.3105775949995735, w0=72.63366336633676, w1=15.976008976342198\n",
      "SubGD iter. 411/499: loss=5.310625791064516, w0=72.62673267326745, w1=15.97388287655646\n",
      "SubGD iter. 412/499: loss=5.310578828885431, w0=72.62673267326745, w1=15.972273215835374\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326745, w1=15.970663555114287\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633676, w1=15.975132254921691\n",
      "SubGD iter. 415/499: loss=5.310623128211339, w0=72.62673267326745, w1=15.973006155135954\n",
      "SubGD iter. 416/499: loss=5.310576812851097, w0=72.62673267326745, w1=15.971396494414867\n",
      "SubGD iter. 417/499: loss=5.310578512884047, w0=72.63366336633676, w1=15.975865194222271\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326745, w1=15.973739094436533\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326745, w1=15.972129433715446\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326745, w1=15.97051977299436\n",
      "SubGD iter. 421/499: loss=5.310584109748107, w0=72.63366336633676, w1=15.974988472801764\n",
      "SubGD iter. 422/499: loss=5.310622691504004, w0=72.62673267326745, w1=15.972862373016026\n",
      "SubGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326745, w1=15.97125271229494\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633676, w1=15.975721412102343\n",
      "SubGD iter. 425/499: loss=5.310624917649846, w0=72.62673267326745, w1=15.973595312316606\n",
      "SubGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326745, w1=15.971985651595519\n",
      "SubGD iter. 427/499: loss=5.310574751788933, w0=72.63366336633676, w1=15.976454351402923\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326745, w1=15.974328251617186\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326745, w1=15.972718590896099\n",
      "SubGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326745, w1=15.971108930175012\n",
      "SubGD iter. 431/499: loss=5.310580348652993, w0=72.63366336633676, w1=15.975577629982416\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326745, w1=15.973451530196678\n",
      "SubGD iter. 433/499: loss=5.310577836997871, w0=72.62673267326745, w1=15.971841869475591\n",
      "SubGD iter. 434/499: loss=5.310575669673406, w0=72.63366336633676, w1=15.976310569282996\n",
      "SubGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326745, w1=15.974184469497258\n",
      "SubGD iter. 436/499: loss=5.3105795224030174, w0=72.62673267326745, w1=15.972574808776171\n",
      "SubGD iter. 437/499: loss=5.310575820963535, w0=72.62673267326745, w1=15.970965148055084\n",
      "SubGD iter. 438/499: loss=5.310581266537468, w0=72.63366336633676, w1=15.975433847862488\n",
      "SubGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326745, w1=15.973307748076751\n",
      "SubGD iter. 440/499: loss=5.310577506368683, w0=72.62673267326745, w1=15.971698087355664\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633676, w1=15.976166787163068\n",
      "SubGD iter. 442/499: loss=5.31062627038102, w0=72.62673267326745, w1=15.97404068737733\n",
      "SubGD iter. 443/499: loss=5.310579191773829, w0=72.62673267326745, w1=15.972431026656244\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326745, w1=15.970821365935157\n",
      "SubGD iter. 445/499: loss=5.310582184421942, w0=72.63366336633676, w1=15.975290065742561\n",
      "SubGD iter. 446/499: loss=5.310623607527843, w0=72.62673267326745, w1=15.973163965956823\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326745, w1=15.971554305235736\n",
      "SubGD iter. 448/499: loss=5.310577505442354, w0=72.63366336633676, w1=15.97602300504314\n",
      "SubGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326745, w1=15.973896905257403\n",
      "SubGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326745, w1=15.972287244536316\n",
      "SubGD iter. 451/499: loss=5.310575159705161, w0=72.62673267326745, w1=15.97067758381523\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633676, w1=15.975146283622633\n",
      "SubGD iter. 453/499: loss=5.310623170820507, w0=72.62673267326745, w1=15.973020183836896\n",
      "SubGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326745, w1=15.971410523115809\n",
      "SubGD iter. 455/499: loss=5.310578423326828, w0=72.63366336633676, w1=15.975879222923213\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326745, w1=15.973753123137476\n",
      "SubGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326745, w1=15.972143462416389\n",
      "SubGD iter. 458/499: loss=5.310574829075974, w0=72.62673267326745, w1=15.970533801695302\n",
      "SubGD iter. 459/499: loss=5.3105840201908885, w0=72.63366336633676, w1=15.975002501502706\n",
      "SubGD iter. 460/499: loss=5.310622734113172, w0=72.62673267326745, w1=15.972876401716968\n",
      "SubGD iter. 461/499: loss=5.310576514481122, w0=72.62673267326745, w1=15.971266740995882\n",
      "SubGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633676, w1=15.975735440803286\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326745, w1=15.973609341017548\n",
      "SubGD iter. 464/499: loss=5.310578199886271, w0=72.62673267326745, w1=15.971999680296461\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633676, w1=15.976468380103865\n",
      "SubGD iter. 466/499: loss=5.310627186404856, w0=72.62673267326745, w1=15.974342280318128\n",
      "SubGD iter. 467/499: loss=5.310579885291417, w0=72.62673267326745, w1=15.972732619597041\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326745, w1=15.971122958875954\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633676, w1=15.975591658683358\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326745, w1=15.97346555889762\n",
      "SubGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326745, w1=15.971855898176534\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633676, w1=15.976324597983938\n",
      "SubGD iter. 473/499: loss=5.310626749697522, w0=72.62673267326745, w1=15.9741984981982\n",
      "SubGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326745, w1=15.972588837477113\n",
      "SubGD iter. 475/499: loss=5.310575853222749, w0=72.62673267326745, w1=15.970979176756027\n",
      "SubGD iter. 476/499: loss=5.310581176980248, w0=72.63366336633676, w1=15.97544787656343\n",
      "SubGD iter. 477/499: loss=5.310624086844346, w0=72.62673267326745, w1=15.973321776777693\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326745, w1=15.971712116056606\n",
      "SubGD iter. 479/499: loss=5.310576498000661, w0=72.63366336633676, w1=15.97618081586401\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326745, w1=15.974054716078273\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326745, w1=15.972445055357186\n",
      "SubGD iter. 482/499: loss=5.310575522593562, w0=72.62673267326745, w1=15.970835394636099\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633676, w1=15.975304094443503\n",
      "SubGD iter. 484/499: loss=5.3106236501370105, w0=72.62673267326745, w1=15.973177994657766\n",
      "SubGD iter. 485/499: loss=5.310577207998708, w0=72.62673267326745, w1=15.971568333936679\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633676, w1=15.976037033744083\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326745, w1=15.973910933958345\n",
      "SubGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326745, w1=15.972301273237258\n",
      "SubGD iter. 489/499: loss=5.310575191964373, w0=72.62673267326745, w1=15.970691612516172\n",
      "SubGD iter. 490/499: loss=5.310583012749197, w0=72.63366336633676, w1=15.975160312323576\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326745, w1=15.973034212537838\n",
      "SubGD iter. 492/499: loss=5.310576877369523, w0=72.62673267326745, w1=15.971424551816751\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633676, w1=15.975893251624155\n",
      "SubGD iter. 494/499: loss=5.310625439575519, w0=72.62673267326745, w1=15.973767151838418\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326745, w1=15.972157491117331\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326745, w1=15.970547830396244\n",
      "SubGD iter. 497/499: loss=5.310583930633669, w0=72.63366336633676, w1=15.975016530203648\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326745, w1=15.97289043041791\n",
      "SubGD iter. 499/499: loss=5.310576546740334, w0=72.62673267326745, w1=15.971280769696824\n",
      "SubGD: execution time=0.015 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:06:52.681157202Z",
     "start_time": "2023-10-05T08:06:52.338496954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd088b34faab4e09a81bcc048e9b376c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.plot_figure(n_iter)>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:18:20.125999874Z",
     "start_time": "2023-10-05T08:18:20.082362821Z"
    }
   },
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic SubGradient Descent algorithm (SubSGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        loss = compute_loss_MAE(y, tx, w)\n",
    "        gradient = np.zeros(tx.shape[1]) # init the gradient for the upcomming batch\n",
    "        for y_batch, tx_batch in batch_iter(y,tx, batch_size):\n",
    "            gradient += compute_subgradient_mae(y_batch, tx_batch, w)\n",
    "\n",
    "        w = w - gamma * gradient/batch_size    \n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        # ***************************************************\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:25:35.884109144Z",
     "start_time": "2023-10-05T08:25:35.840860105Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=-0.9016322644127577\n",
      "SubSGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=-1.1868067589620521\n",
      "SubSGD iter. 2/499: loss=72.66780585492639, w0=2.0999999999999996, w1=-0.6911804984932197\n",
      "SubSGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=-0.9626606804757989\n",
      "SubSGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=-1.3285814112061962\n",
      "SubSGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=-1.664944368882273\n",
      "SubSGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=-2.1919014046511207\n",
      "SubSGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=-3.0935336690638784\n",
      "SubSGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=-3.2983671889391015\n",
      "SubSGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=-3.6642879196694986\n",
      "SubSGD iter. 10/499: loss=67.06780585492639, w0=7.700000000000001, w1=-2.7591128164824354\n",
      "SubSGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=-1.6339828563521284\n",
      "SubSGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=-2.330567607592891\n",
      "SubSGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=-1.4972300250333141\n",
      "SubSGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=-1.232444915842546\n",
      "SubSGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=-0.3991073332829691\n",
      "SubSGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=0.21388516390059065\n",
      "SubSGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=0.20012996792510002\n",
      "SubSGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=0.8147909490554868\n",
      "SubSGD iter. 19/499: loss=60.76780585492637, w0=13.999999999999995, w1=1.1592591959259542\n",
      "SubSGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.7904245359208066\n",
      "SubSGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=2.433354234684602\n",
      "SubSGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=3.0256721959527804\n",
      "SubSGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=4.143982416564262\n",
      "SubSGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=4.2325099300533875\n",
      "SubSGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=4.656051183107403\n",
      "SubSGD iter. 26/499: loss=55.867805854926374, w0=18.89999999999999, w1=4.921534885331633\n",
      "SubSGD iter. 27/499: loss=55.16780585492638, w0=19.59999999999999, w1=4.489742307048213\n",
      "SubSGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=5.108875702992879\n",
      "SubSGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=4.772512745316803\n",
      "SubSGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=2.0022130484278136\n",
      "SubSGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=3.1273430085581206\n",
      "SubSGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=3.576002047342831\n",
      "SubSGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=3.56224685136734\n",
      "SubSGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=3.442355037879009\n",
      "SubSGD iter. 35/499: loss=49.567805854926384, w0=25.199999999999985, w1=3.418389198075918\n",
      "SubSGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=4.164374851663315\n",
      "SubSGD iter. 37/499: loss=48.167805854926385, w0=26.599999999999984, w1=3.9595413317880914\n",
      "SubSGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=4.161978221829091\n",
      "SubSGD iter. 39/499: loss=46.767805854926394, w0=27.999999999999982, w1=5.356717058933516\n",
      "SubSGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=4.538058689127888\n",
      "SubSGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=5.394280741632426\n",
      "SubSGD iter. 42/499: loss=44.667805854926385, w0=30.09999999999998, w1=4.586167000590493\n",
      "SubSGD iter. 43/499: loss=43.96780585492639, w0=30.79999999999998, w1=3.77805325954856\n",
      "SubSGD iter. 44/499: loss=43.267805854926394, w0=31.49999999999998, w1=4.3919676729261425\n",
      "SubSGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=4.7356802941095335\n",
      "SubSGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=5.231306554578366\n",
      "SubSGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=6.323931068904161\n",
      "SubSGD iter. 48/499: loss=40.46780585492638, w0=34.29999999999999, w1=6.790513194870735\n",
      "SubSGD iter. 49/499: loss=39.76780585492638, w0=34.99999999999999, w1=6.217503111856112\n",
      "SubSGD iter. 50/499: loss=39.06780585492638, w0=35.699999999999996, w1=5.315870847443354\n",
      "SubSGD iter. 51/499: loss=38.367805854926374, w0=36.4, w1=4.87618244235817\n",
      "SubSGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=5.429388358434283\n",
      "SubSGD iter. 53/499: loss=36.96780585492637, w0=37.800000000000004, w1=5.631825248475281\n",
      "SubSGD iter. 54/499: loss=36.267805854926365, w0=38.50000000000001, w1=6.15988106398593\n",
      "SubSGD iter. 55/499: loss=35.56780585492636, w0=39.20000000000001, w1=5.918901149319281\n",
      "SubSGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=5.240554153344738\n",
      "SubSGD iter. 57/499: loss=34.167805854926364, w0=40.600000000000016, w1=5.8328721146129165\n",
      "SubSGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=3.0625724177239273\n",
      "SubSGD iter. 59/499: loss=32.76780585492636, w0=42.00000000000002, w1=3.037274652335086\n",
      "SubSGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=3.0048456445677263\n",
      "SubSGD iter. 61/499: loss=31.36780585492635, w0=43.40000000000003, w1=2.787588640387976\n",
      "SubSGD iter. 62/499: loss=30.667805854926346, w0=44.10000000000003, w1=2.2520689575617707\n",
      "SubSGD iter. 63/499: loss=29.967805854926343, w0=44.80000000000003, w1=2.417875217129268\n",
      "SubSGD iter. 64/499: loss=29.26780585492634, w0=45.500000000000036, w1=1.8597515133802287\n",
      "SubSGD iter. 65/499: loss=28.567805854926338, w0=46.20000000000004, w1=3.0280823138976976\n",
      "SubSGD iter. 66/499: loss=27.867805854926335, w0=46.90000000000004, w1=2.2887495144537535\n",
      "SubSGD iter. 67/499: loss=27.167805854926332, w0=47.600000000000044, w1=1.8629633671630144\n",
      "SubSGD iter. 68/499: loss=26.467805854926336, w0=48.30000000000005, w1=2.0920850594497726\n",
      "SubSGD iter. 69/499: loss=25.767805854926333, w0=49.00000000000005, w1=1.2032532352832792\n",
      "SubSGD iter. 70/499: loss=25.07489988690716, w0=49.70000000000005, w1=1.5469658564666697\n",
      "SubSGD iter. 71/499: loss=24.377109963556112, w0=50.400000000000055, w1=1.6607861832809825\n",
      "SubSGD iter. 72/499: loss=23.69066060210616, w0=51.10000000000006, w1=0.9740915477748459\n",
      "SubSGD iter. 73/499: loss=23.065037023736313, w0=51.80000000000006, w1=2.1424223482923144\n",
      "SubSGD iter. 74/499: loss=22.31781707697547, w0=52.500000000000064, w1=3.2262977449836163\n",
      "SubSGD iter. 75/499: loss=21.588903419630263, w0=53.20000000000007, w1=3.263585056601486\n",
      "SubSGD iter. 76/499: loss=20.91407465858182, w0=53.90000000000007, w1=2.9068595382456492\n",
      "SubSGD iter. 77/499: loss=20.261200856724923, w0=54.60000000000007, w1=2.3030934682791355\n",
      "SubSGD iter. 78/499: loss=19.693918553825615, w0=55.300000000000075, w1=3.0982631933964826\n",
      "SubSGD iter. 79/499: loss=18.950692829412553, w0=56.00000000000008, w1=2.5401394896474434\n",
      "SubSGD iter. 80/499: loss=18.44091671261355, w0=55.300000000000075, w1=3.348253230689376\n",
      "SubSGD iter. 81/499: loss=18.919433104639037, w0=56.00000000000008, w1=3.228361417201045\n",
      "SubSGD iter. 82/499: loss=18.308726611102696, w0=55.300000000000075, w1=4.0644002744738605\n",
      "SubSGD iter. 83/499: loss=18.85496666610315, w0=56.00000000000008, w1=3.4416138676535684\n",
      "SubSGD iter. 84/499: loss=18.271418239005456, w0=56.70000000000008, w1=4.056274848783955\n",
      "SubSGD iter. 85/499: loss=17.558001335863505, w0=57.400000000000084, w1=4.7744381728151435\n",
      "SubSGD iter. 86/499: loss=16.832570277566283, w0=58.10000000000009, w1=4.220331098818563\n",
      "SubSGD iter. 87/499: loss=16.335648559643836, w0=58.80000000000009, w1=4.38613735838606\n",
      "SubSGD iter. 88/499: loss=15.75125102258712, w0=59.50000000000009, w1=4.392565239264407\n",
      "SubSGD iter. 89/499: loss=15.22740720187413, w0=60.200000000000095, w1=4.603729633830611\n",
      "SubSGD iter. 90/499: loss=14.672231683156934, w0=60.9000000000001, w1=5.218390614960997\n",
      "SubSGD iter. 91/499: loss=13.998048755922099, w0=61.6000000000001, w1=4.787379930212383\n",
      "SubSGD iter. 92/499: loss=13.713441763403488, w0=62.300000000000104, w1=5.237559676418677\n",
      "SubSGD iter. 93/499: loss=13.131092225225874, w0=61.6000000000001, w1=6.421561876812568\n",
      "SubSGD iter. 94/499: loss=13.148659187479634, w0=62.300000000000104, w1=6.765274497995959\n",
      "SubSGD iter. 95/499: loss=12.570221489114179, w0=63.00000000000011, w1=7.100502553299214\n",
      "SubSGD iter. 96/499: loss=12.01759179354221, w0=63.70000000000011, w1=6.443621585546251\n",
      "SubSGD iter. 97/499: loss=11.879614506008746, w0=64.4000000000001, w1=7.401756676448058\n",
      "SubSGD iter. 98/499: loss=11.108579731070371, w0=65.10000000000011, w1=7.929812491958707\n",
      "SubSGD iter. 99/499: loss=10.526794582955635, w0=65.80000000000011, w1=8.357329577176126\n",
      "SubSGD iter. 100/499: loss=10.00097750992393, w0=66.50000000000011, w1=7.7730170396302345\n",
      "SubSGD iter. 101/499: loss=9.974677374512842, w0=65.80000000000011, w1=8.05556485871646\n",
      "SubSGD iter. 102/499: loss=10.138686331413167, w0=66.50000000000011, w1=8.66855735590002\n",
      "SubSGD iter. 103/499: loss=9.547985527178266, w0=67.20000000000012, w1=8.710883912503512\n",
      "SubSGD iter. 104/499: loss=9.243702114541687, w0=67.90000000000012, w1=8.763044258033222\n",
      "SubSGD iter. 105/499: loss=8.951397029615784, w0=68.60000000000012, w1=9.01332537449528\n",
      "SubSGD iter. 106/499: loss=8.596884428649574, w0=67.90000000000012, w1=9.383593465024076\n",
      "SubSGD iter. 107/499: loss=8.649671794170054, w0=67.20000000000012, w1=9.926609926389174\n",
      "SubSGD iter. 108/499: loss=8.67602100133592, w0=67.90000000000012, w1=9.556341835860378\n",
      "SubSGD iter. 109/499: loss=8.568733269824053, w0=68.60000000000012, w1=10.751080672964804\n",
      "SubSGD iter. 110/499: loss=7.780252606656656, w0=67.90000000000012, w1=11.550911407027513\n",
      "SubSGD iter. 111/499: loss=7.7667615250296755, w0=68.60000000000012, w1=13.090973745414699\n",
      "SubSGD iter. 112/499: loss=6.997889534327171, w0=69.30000000000013, w1=13.722139085409552\n",
      "SubSGD iter. 113/499: loss=6.5202238643491475, w0=70.00000000000013, w1=13.689710077642191\n",
      "SubSGD iter. 114/499: loss=6.2368102026790115, w0=70.70000000000013, w1=12.889879343579482\n",
      "SubSGD iter. 115/499: loss=6.272851940264793, w0=71.40000000000013, w1=14.070684044279378\n",
      "SubSGD iter. 116/499: loss=5.706609888759956, w0=72.10000000000014, w1=14.097056694904198\n",
      "SubSGD iter. 117/499: loss=5.565902962490395, w0=72.80000000000014, w1=10.723913017314521\n",
      "SubSGD iter. 118/499: loss=6.823195220496666, w0=73.50000000000014, w1=12.089282469978132\n",
      "SubSGD iter. 119/499: loss=6.154126937599422, w0=72.80000000000014, w1=12.86010553604632\n",
      "SubSGD iter. 120/499: loss=5.860213861380681, w0=72.10000000000014, w1=13.349789069442908\n",
      "SubSGD iter. 121/499: loss=5.7800846261901135, w0=71.40000000000013, w1=13.593815894152058\n",
      "SubSGD iter. 122/499: loss=5.849354171658731, w0=72.10000000000014, w1=13.790160487669286\n",
      "SubSGD iter. 123/499: loss=5.6453165637491365, w0=71.40000000000013, w1=13.78373260679094\n",
      "SubSGD iter. 124/499: loss=5.790542575380464, w0=72.10000000000014, w1=14.578902331908287\n",
      "SubSGD iter. 125/499: loss=5.467869559530605, w0=71.40000000000013, w1=14.779651868957538\n",
      "SubSGD iter. 126/499: loss=5.532133778773355, w0=72.10000000000014, w1=16.01896816993554\n",
      "SubSGD iter. 127/499: loss=5.338084696807315, w0=72.80000000000014, w1=16.45889261281972\n",
      "SubSGD iter. 128/499: loss=5.325585578916297, w0=72.10000000000014, w1=17.347724436986216\n",
      "SubSGD iter. 129/499: loss=5.458258451015577, w0=72.80000000000014, w1=16.553355092495437\n",
      "SubSGD iter. 130/499: loss=5.335886308537291, w0=72.10000000000014, w1=17.06185621023527\n",
      "SubSGD iter. 131/499: loss=5.410833620592303, w0=72.80000000000014, w1=17.793820803742904\n",
      "SubSGD iter. 132/499: loss=5.534017965864255, w0=73.50000000000014, w1=18.439968037130125\n",
      "SubSGD iter. 133/499: loss=5.748421237922946, w0=74.20000000000014, w1=19.56509799726043\n",
      "SubSGD iter. 134/499: loss=6.2612038313992056, w0=73.50000000000014, w1=19.36266110721943\n",
      "SubSGD iter. 135/499: loss=6.049565890899443, w0=72.80000000000014, w1=19.101948885837565\n",
      "SubSGD iter. 136/499: loss=5.907954316470591, w0=73.50000000000014, w1=19.069519878070206\n",
      "SubSGD iter. 137/499: loss=5.941380019269801, w0=72.80000000000014, w1=19.414696965795205\n",
      "SubSGD iter. 138/499: loss=6.022437805135842, w0=72.10000000000014, w1=19.846489544078626\n",
      "SubSGD iter. 139/499: loss=6.219924949817132, w0=72.80000000000014, w1=19.0622568160393\n",
      "SubSGD iter. 140/499: loss=5.893999997212215, w0=73.50000000000014, w1=19.02756595669783\n",
      "SubSGD iter. 141/499: loss=5.927204434954234, w0=72.80000000000014, w1=19.58568966044687\n",
      "SubSGD iter. 142/499: loss=6.087670168327007, w0=73.50000000000014, w1=18.731843390021602\n",
      "SubSGD iter. 143/499: loss=5.831516921170553, w0=72.80000000000014, w1=18.675533774976298\n",
      "SubSGD iter. 144/499: loss=5.76773600281164, w0=72.10000000000014, w1=17.61761846814874\n",
      "SubSGD iter. 145/499: loss=5.510918603256378, w0=71.40000000000013, w1=16.422879631044317\n",
      "SubSGD iter. 146/499: loss=5.44249216279004, w0=72.10000000000014, w1=16.026903387822486\n",
      "SubSGD iter. 147/499: loss=5.338146328542394, w0=72.80000000000014, w1=15.242670659783162\n",
      "SubSGD iter. 148/499: loss=5.342469686221607, w0=73.50000000000014, w1=14.65835812223727\n",
      "SubSGD iter. 149/499: loss=5.417834457436895, w0=74.20000000000014, w1=14.085348039222648\n",
      "SubSGD iter. 150/499: loss=5.584301262665584, w0=73.50000000000014, w1=14.103927498121525\n",
      "SubSGD iter. 151/499: loss=5.489586677822963, w0=74.20000000000014, w1=14.74685719688532\n",
      "SubSGD iter. 152/499: loss=5.503070864298278, w0=73.50000000000014, w1=15.09203428461032\n",
      "SubSGD iter. 153/499: loss=5.385271308973099, w0=74.20000000000014, w1=14.742096353130002\n",
      "SubSGD iter. 154/499: loss=5.503492186166603, w0=73.50000000000014, w1=15.092433286498393\n",
      "SubSGD iter. 155/499: loss=5.385249897885984, w0=72.80000000000014, w1=15.336460111207543\n",
      "SubSGD iter. 156/499: loss=5.337120125991197, w0=72.10000000000014, w1=15.417274416696255\n",
      "SubSGD iter. 157/499: loss=5.360111011457115, w0=72.80000000000014, w1=16.258464584920898\n",
      "SubSGD iter. 158/499: loss=5.316919750203497, w0=72.10000000000014, w1=15.524188841429098\n",
      "SubSGD iter. 159/499: loss=5.352575837708446, w0=71.40000000000013, w1=16.076160273366778\n",
      "SubSGD iter. 160/499: loss=5.434219440062823, w0=70.70000000000013, w1=16.515848678451963\n",
      "SubSGD iter. 161/499: loss=5.61743774491585, w0=71.40000000000013, w1=16.16551174508357\n",
      "SubSGD iter. 162/499: loss=5.43451158892767, w0=72.10000000000014, w1=15.795243654554774\n",
      "SubSGD iter. 163/499: loss=5.342676627309493, w0=71.40000000000013, w1=15.506746389997359\n",
      "SubSGD iter. 164/499: loss=5.453306792497246, w0=72.10000000000014, w1=15.301912870122136\n",
      "SubSGD iter. 165/499: loss=5.36903604125299, w0=72.80000000000014, w1=14.682771714707753\n",
      "SubSGD iter. 166/499: loss=5.3990366307402615, w0=73.50000000000014, w1=14.582494615467544\n",
      "SubSGD iter. 167/499: loss=5.425286393440182, w0=72.80000000000014, w1=15.205281022287837\n",
      "SubSGD iter. 168/499: loss=5.344792286044307, w0=73.50000000000014, w1=16.217959363640603\n",
      "SubSGD iter. 169/499: loss=5.366330177621691, w0=72.80000000000014, w1=15.60404495026302\n",
      "SubSGD iter. 170/499: loss=5.322496655723045, w0=73.50000000000014, w1=16.483217262493827\n",
      "SubSGD iter. 171/499: loss=5.3886026250153565, w0=74.20000000000014, w1=16.7533557127835\n",
      "SubSGD iter. 172/499: loss=5.537364317662443, w0=74.90000000000015, w1=16.997668436492635\n",
      "SubSGD iter. 173/499: loss=5.744061497812501, w0=74.20000000000014, w1=15.791652146722187\n",
      "SubSGD iter. 174/499: loss=5.4666165885684315, w0=73.50000000000014, w1=14.83351705582038\n",
      "SubSGD iter. 175/499: loss=5.402518928844442, w0=74.20000000000014, w1=14.483579124340062\n",
      "SubSGD iter. 176/499: loss=5.530768323445897, w0=73.50000000000014, w1=14.833916057708453\n",
      "SubSGD iter. 177/499: loss=5.402488021584491, w0=74.20000000000014, w1=14.947736384522766\n",
      "SubSGD iter. 178/499: loss=5.4870734860392, w0=73.50000000000014, w1=15.499707816460445\n",
      "SubSGD iter. 179/499: loss=5.367471630269388, w0=72.80000000000014, w1=15.93939622154563\n",
      "SubSGD iter. 180/499: loss=5.312908202970109, w0=72.10000000000014, w1=16.324680361873757\n",
      "SubSGD iter. 181/499: loss=5.344894700555266, w0=72.80000000000014, w1=16.47757411898834\n",
      "SubSGD iter. 182/499: loss=5.32762271730835, w0=73.50000000000014, w1=16.905091204205757\n",
      "SubSGD iter. 183/499: loss=5.436411917313559, w0=72.80000000000014, w1=16.48154995115174\n",
      "SubSGD iter. 184/499: loss=5.328056264810748, w0=72.10000000000014, w1=15.877017845297582\n",
      "SubSGD iter. 185/499: loss=5.340748965170233, w0=72.80000000000014, w1=16.224984891687996\n",
      "SubSGD iter. 186/499: loss=5.316203069187609, w0=73.50000000000014, w1=15.198052694095463\n",
      "SubSGD iter. 187/499: loss=5.380342883120882, w0=72.80000000000014, w1=14.592649542651477\n",
      "SubSGD iter. 188/499: loss=5.410890117065636, w0=73.50000000000014, w1=15.831965843629478\n",
      "SubSGD iter. 189/499: loss=5.362094336255849, w0=72.80000000000014, w1=14.77405053680192\n",
      "SubSGD iter. 190/499: loss=5.387106952552358, w0=73.50000000000014, w1=15.426107434244374\n",
      "SubSGD iter. 191/499: loss=5.370042757198094, w0=74.20000000000014, w1=15.979313350320487\n",
      "SubSGD iter. 192/499: loss=5.475156875899117, w0=74.90000000000015, w1=15.795847571270068\n",
      "SubSGD iter. 193/499: loss=5.627572656348719, w0=75.60000000000015, w1=15.822220221894888\n",
      "SubSGD iter. 194/499: loss=5.864109110759474, w0=74.90000000000015, w1=16.02296975894414\n",
      "SubSGD iter. 195/499: loss=5.640034983604356, w0=74.20000000000014, w1=16.817339103434918\n",
      "SubSGD iter. 196/499: loss=5.5436946781394525, w0=73.50000000000014, w1=17.243125250725658\n",
      "SubSGD iter. 197/499: loss=5.493759666256311, w0=72.80000000000014, w1=17.186815635680354\n",
      "SubSGD iter. 198/499: loss=5.424360845890094, w0=73.50000000000014, w1=18.311945595810663\n",
      "SubSGD iter. 199/499: loss=5.71400517176712, w0=72.80000000000014, w1=18.924743766812075\n",
      "SubSGD iter. 200/499: loss=5.8472571709275405, w0=72.10000000000014, w1=18.129574041694728\n",
      "SubSGD iter. 201/499: loss=5.637025103479791, w0=71.40000000000013, w1=18.619257575091314\n",
      "SubSGD iter. 202/499: loss=5.877860902639996, w0=70.70000000000013, w1=18.070863071333502\n",
      "SubSGD iter. 203/499: loss=5.880014409058357, w0=71.40000000000013, w1=18.49838015655092\n",
      "SubSGD iter. 204/499: loss=5.841175901493498, w0=72.10000000000014, w1=18.776507125352605\n",
      "SubSGD iter. 205/499: loss=5.834281268590072, w0=71.40000000000013, w1=18.897011676831646\n",
      "SubSGD iter. 206/499: loss=5.96429688324697, w0=72.10000000000014, w1=18.54389592526433\n",
      "SubSGD iter. 207/499: loss=5.759228882236594, w0=71.40000000000013, w1=17.924762529319665\n",
      "SubSGD iter. 208/499: loss=5.677034863213327, w0=70.70000000000013, w1=17.178776875732268\n",
      "SubSGD iter. 209/499: loss=5.69607688173424, w0=70.00000000000013, w1=17.668460409128855\n",
      "SubSGD iter. 210/499: loss=6.032478319773878, w0=70.70000000000013, w1=18.283121390259243\n",
      "SubSGD iter. 211/499: loss=5.937861912466468, w0=71.40000000000013, w1=17.658305290448297\n",
      "SubSGD iter. 212/499: loss=5.608791056162712, w0=72.10000000000014, w1=16.863935945957518\n",
      "SubSGD iter. 213/499: loss=5.385112179026004, w0=71.40000000000013, w1=16.440394692903503\n",
      "SubSGD iter. 214/499: loss=5.443167491088153, w0=70.70000000000013, w1=15.944768432434671\n",
      "SubSGD iter. 215/499: loss=5.6136445367150465, w0=70.00000000000013, w1=16.786186942160615\n",
      "SubSGD iter. 216/499: loss=5.917175586737202, w0=69.30000000000013, w1=15.906456812328138\n",
      "SubSGD iter. 217/499: loss=6.190312104103589, w0=70.00000000000013, w1=16.049787402922654\n",
      "SubSGD iter. 218/499: loss=5.857082269320715, w0=70.70000000000013, w1=16.315271105146884\n",
      "SubSGD iter. 219/499: loss=5.6115489395598335, w0=71.40000000000013, w1=15.189338691645712\n",
      "SubSGD iter. 220/499: loss=5.4821385565495735, w0=72.10000000000014, w1=15.299638725106504\n",
      "SubSGD iter. 221/499: loss=5.369216234820735, w0=72.80000000000014, w1=15.577765693908193\n",
      "SubSGD iter. 222/499: loss=5.323932813037976, w0=72.10000000000014, w1=14.920582990713207\n",
      "SubSGD iter. 223/499: loss=5.4169830490618915, w0=71.40000000000013, w1=15.040474804201539\n",
      "SubSGD iter. 224/499: loss=5.498622423138534, w0=72.10000000000014, w1=12.27017510731255\n",
      "SubSGD iter. 225/499: loss=6.183168044975659, w0=72.80000000000014, w1=12.558672371869966\n",
      "SubSGD iter. 226/499: loss=5.982065760667145, w0=72.10000000000014, w1=13.116796075619005\n",
      "SubSGD iter. 227/499: loss=5.858659169318276, w0=72.80000000000014, w1=13.973018128123543\n",
      "SubSGD iter. 228/499: loss=5.521934714055271, w0=72.10000000000014, w1=14.508537810949749\n",
      "SubSGD iter. 229/499: loss=5.478587972172305, w0=72.80000000000014, w1=14.484571971146657\n",
      "SubSGD iter. 230/499: loss=5.426458461659519, w0=73.50000000000014, w1=13.787987219905894\n",
      "SubSGD iter. 231/499: loss=5.553900348602138, w0=72.80000000000014, w1=14.877782242979114\n",
      "SubSGD iter. 232/499: loss=5.3746153798788, w0=72.10000000000014, w1=15.303568390269854\n",
      "SubSGD iter. 233/499: loss=5.3689048648856765, w0=71.40000000000013, w1=15.735360968553275\n",
      "SubSGD iter. 234/499: loss=5.441688194079302, w0=72.10000000000014, w1=17.27542330694046\n",
      "SubSGD iter. 235/499: loss=5.4448939833938015, w0=71.40000000000013, w1=17.219113691895156\n",
      "SubSGD iter. 236/499: loss=5.513218577684424, w0=70.70000000000013, w1=16.33938356206268\n",
      "SubSGD iter. 237/499: loss=5.611810624835824, w0=70.00000000000013, w1=16.724667702390807\n",
      "SubSGD iter. 238/499: loss=5.911547181828736, w0=70.70000000000013, w1=16.700701862587717\n",
      "SubSGD iter. 239/499: loss=5.633415438286971, w0=71.40000000000013, w1=16.270370274126982\n",
      "SubSGD iter. 240/499: loss=5.437209146555095, w0=72.10000000000014, w1=15.293572795185064\n",
      "SubSGD iter. 241/499: loss=5.369716273675667, w0=72.80000000000014, w1=15.733497238069246\n",
      "SubSGD iter. 242/499: loss=5.316253072618252, w0=72.10000000000014, w1=14.64087272374345\n",
      "SubSGD iter. 243/499: loss=5.458429798949438, w0=72.80000000000014, w1=15.653551065096218\n",
      "SubSGD iter. 244/499: loss=5.319802280058896, w0=73.50000000000014, w1=15.12891814038448\n",
      "SubSGD iter. 245/499: loss=5.383534782886575, w0=72.80000000000014, w1=15.122490259506133\n",
      "SubSGD iter. 246/499: loss=5.35056567287205, w0=72.10000000000014, w1=15.469574245126124\n",
      "SubSGD iter. 247/499: loss=5.356424995325714, w0=71.40000000000013, w1=14.812391541931138\n",
      "SubSGD iter. 248/499: loss=5.526483310314909, w0=70.70000000000013, w1=13.85425645102933\n",
      "SubSGD iter. 249/499: loss=5.957746388389568, w0=71.40000000000013, w1=14.124394901319\n",
      "SubSGD iter. 250/499: loss=5.691909172076627, w0=70.70000000000013, w1=13.06799366994011\n",
      "SubSGD iter. 251/499: loss=6.209902205598326, w0=71.40000000000013, w1=13.074421550818457\n",
      "SubSGD iter. 252/499: loss=6.0232464152011875, w0=72.10000000000014, w1=13.501938636035876\n",
      "SubSGD iter. 253/499: loss=5.729712277116749, w0=72.80000000000014, w1=14.707954925806323\n",
      "SubSGD iter. 254/499: loss=5.3957453130254684, w0=73.50000000000014, w1=13.937131859738136\n",
      "SubSGD iter. 255/499: loss=5.5210049676160295, w0=74.20000000000014, w1=14.580061558501932\n",
      "SubSGD iter. 256/499: loss=5.519384293206503, w0=73.50000000000014, w1=14.955647396788502\n",
      "SubSGD iter. 257/499: loss=5.393058537028932, w0=74.20000000000014, w1=15.496604448759644\n",
      "SubSGD iter. 258/499: loss=5.467672517863539, w0=73.50000000000014, w1=15.208107184202229\n",
      "SubSGD iter. 259/499: loss=5.379878673505092, w0=72.80000000000014, w1=14.659712680444418\n",
      "SubSGD iter. 260/499: loss=5.402050329337309, w0=72.10000000000014, w1=14.653284799566071\n",
      "SubSGD iter. 261/499: loss=5.456539105335744, w0=71.40000000000013, w1=13.596883568187181\n",
      "SubSGD iter. 262/499: loss=5.848396802953811, w0=72.10000000000014, w1=12.569951370594648\n",
      "SubSGD iter. 263/499: loss=6.0586138383168375, w0=71.40000000000013, w1=13.096908406363497\n",
      "SubSGD iter. 264/499: loss=6.015376470974585, w0=72.10000000000014, w1=13.475448189720229\n",
      "SubSGD iter. 265/499: loss=5.738178266839873, w0=72.80000000000014, w1=14.600578149850536\n",
      "SubSGD iter. 266/499: loss=5.4097844560695005, w0=72.10000000000014, w1=15.170538274386521\n",
      "SubSGD iter. 267/499: loss=5.382884009200009, w0=71.40000000000013, w1=16.011956784112467\n",
      "SubSGD iter. 268/499: loss=5.4350873699377305, w0=70.70000000000013, w1=15.588415531058452\n",
      "SubSGD iter. 269/499: loss=5.626708298200303, w0=70.00000000000013, w1=14.393676693954026\n",
      "SubSGD iter. 270/499: loss=6.061034364773573, w0=70.70000000000013, w1=15.11869688403483\n",
      "SubSGD iter. 271/499: loss=5.666425791181739, w0=70.00000000000013, w1=15.5588921387418\n",
      "SubSGD iter. 272/499: loss=5.866438311695199, w0=70.70000000000013, w1=15.61105248427151\n",
      "SubSGD iter. 273/499: loss=5.625447899230226, w0=71.40000000000013, w1=16.73618244440182\n",
      "SubSGD iter. 274/499: loss=5.458261581253735, w0=72.10000000000014, w1=17.014309413203506\n",
      "SubSGD iter. 275/499: loss=5.404505680247996, w0=71.40000000000013, w1=17.561471715437378\n",
      "SubSGD iter. 276/499: loss=5.585091632401382, w0=70.70000000000013, w1=17.80549854014653\n",
      "SubSGD iter. 277/499: loss=5.815290841427666, w0=70.00000000000013, w1=17.381957287092515\n",
      "SubSGD iter. 278/499: loss=5.986763701760681, w0=70.70000000000013, w1=17.91165826115463\n",
      "SubSGD iter. 279/499: loss=5.840063287158402, w0=71.40000000000013, w1=18.535146383440807\n",
      "SubSGD iter. 280/499: loss=5.852334057350021, w0=70.70000000000013, w1=17.340407546336383\n",
      "SubSGD iter. 281/499: loss=5.721891145054557, w0=70.00000000000013, w1=17.28409793129108\n",
      "SubSGD iter. 282/499: loss=5.971732480020812, w0=70.70000000000013, w1=16.853087246542465\n",
      "SubSGD iter. 283/499: loss=5.649622195227777, w0=71.40000000000013, w1=16.502750313174072\n",
      "SubSGD iter. 284/499: loss=5.445724805044802, w0=72.10000000000014, w1=16.149634561606756\n",
      "SubSGD iter. 285/499: loss=5.339446632867946, w0=71.40000000000013, w1=16.58932296669194\n",
      "SubSGD iter. 286/499: loss=5.450082668961132, w0=72.10000000000014, w1=16.462308521463918\n",
      "SubSGD iter. 287/499: loss=5.351460635459926, w0=71.40000000000013, w1=15.405907290085027\n",
      "SubSGD iter. 288/499: loss=5.461234943656404, w0=72.10000000000014, w1=14.606076556022318\n",
      "SubSGD iter. 289/499: loss=5.463730192939254, w0=72.80000000000014, w1=15.11526239996857\n",
      "SubSGD iter. 290/499: loss=5.351126360984541, w0=73.50000000000014, w1=15.359575123677704\n",
      "SubSGD iter. 291/499: loss=5.372885492026798, w0=72.80000000000014, w1=14.829874149615588\n",
      "SubSGD iter. 292/499: loss=5.380271660612448, w0=72.10000000000014, w1=15.186599667971425\n",
      "SubSGD iter. 293/499: loss=5.381014040793368, w0=72.80000000000014, w1=14.820678937241027\n",
      "SubSGD iter. 294/499: loss=5.381370112797586, w0=72.10000000000014, w1=15.251689621989641\n",
      "SubSGD iter. 295/499: loss=5.374123477368471, w0=71.40000000000013, w1=14.045673332219193\n",
      "SubSGD iter. 296/499: loss=5.7137573215900135, w0=70.70000000000013, w1=14.535356865615782\n",
      "SubSGD iter. 297/499: loss=5.780122266740143, w0=71.40000000000013, w1=14.823854130173197\n",
      "SubSGD iter. 298/499: loss=5.5248104100929485, w0=70.70000000000013, w1=15.33235524791303\n",
      "SubSGD iter. 299/499: loss=5.642170726056655, w0=70.00000000000013, w1=14.997127192609776\n",
      "SubSGD iter. 300/499: loss=5.941824035024771, w0=70.70000000000013, w1=15.643274425996996\n",
      "SubSGD iter. 301/499: loss=5.623653819356385, w0=71.40000000000013, w1=14.431851538070816\n",
      "SubSGD iter. 302/499: loss=5.610659641449401, w0=72.10000000000014, w1=14.871775980954999\n",
      "SubSGD iter. 303/499: loss=5.423687351011199, w0=71.40000000000013, w1=14.693423859575184\n",
      "SubSGD iter. 304/499: loss=5.549763162765806, w0=72.10000000000014, w1=15.33635355833898\n",
      "SubSGD iter. 305/499: loss=5.366307107766888, w0=72.80000000000014, w1=14.782246484342398\n",
      "SubSGD iter. 306/499: loss=5.386035783858902, w0=73.50000000000014, w1=14.9787665671209\n",
      "SubSGD iter. 307/499: loss=5.391349438021915, w0=72.80000000000014, w1=14.728485450658843\n",
      "SubSGD iter. 308/499: loss=5.393062077759059, w0=72.10000000000014, w1=14.23285919019001\n",
      "SubSGD iter. 309/499: loss=5.532088814843073, w0=72.80000000000014, w1=14.248166856902147\n",
      "SubSGD iter. 310/499: loss=5.467111756515227, w0=73.50000000000014, w1=14.22958739800327\n",
      "SubSGD iter. 311/499: loss=5.469383860756109, w0=74.20000000000014, w1=12.875630503970042\n",
      "SubSGD iter. 312/499: loss=5.87952917569767, w0=73.50000000000014, w1=12.869202623091695\n",
      "SubSGD iter. 313/499: loss=5.8418222231844625, w0=72.80000000000014, w1=13.053363401410015\n",
      "SubSGD iter. 314/499: loss=5.789237112116618, w0=73.50000000000014, w1=12.469050863864124\n",
      "SubSGD iter. 315/499: loss=5.998819026543939, w0=74.20000000000014, w1=13.56167537818992\n",
      "SubSGD iter. 316/499: loss=5.684242410631372, w0=74.90000000000015, w1=14.175589791567502\n",
      "SubSGD iter. 317/499: loss=5.733806505000596, w0=74.20000000000014, w1=14.06528975810671\n",
      "SubSGD iter. 318/499: loss=5.587512504456809, w0=73.50000000000014, w1=13.472971796838532\n",
      "SubSGD iter. 319/499: loss=5.640353111197149, w0=74.20000000000014, w1=13.817440043709\n",
      "SubSGD iter. 320/499: loss=5.630674338123372, w0=73.50000000000014, w1=14.430238214710412\n",
      "SubSGD iter. 321/499: loss=5.441771415131762, w0=74.20000000000014, w1=14.077122463143098\n",
      "SubSGD iter. 322/499: loss=5.585618140892133, w0=73.50000000000014, w1=14.913161320415915\n",
      "SubSGD iter. 323/499: loss=5.3963495695930135, w0=72.80000000000014, w1=15.3528497255011\n",
      "SubSGD iter. 324/499: loss=5.336224436092717, w0=73.50000000000014, w1=15.334270266602223\n",
      "SubSGD iter. 325/499: loss=5.374053801693014, w0=74.20000000000014, w1=15.536707156643223\n",
      "SubSGD iter. 326/499: loss=5.467007634642332, w0=73.50000000000014, w1=14.330690866872775\n",
      "SubSGD iter. 327/499: loss=5.454712647522107, w0=72.80000000000014, w1=15.17210937659872\n",
      "SubSGD iter. 328/499: loss=5.346852869231447, w0=73.50000000000014, w1=15.713066428569862\n",
      "SubSGD iter. 329/499: loss=5.363424265845195, w0=74.20000000000014, w1=14.859220158144593\n",
      "SubSGD iter. 330/499: loss=5.493828190082492, w0=73.50000000000014, w1=15.402236619509692\n",
      "SubSGD iter. 331/499: loss=5.37091583703894, w0=74.20000000000014, w1=16.942298957896877\n",
      "SubSGD iter. 332/499: loss=5.558838283241571, w0=74.90000000000015, w1=16.6059360002208\n",
      "SubSGD iter. 333/499: loss=5.691540219114767, w0=75.60000000000015, w1=15.909351248980037\n",
      "SubSGD iter. 334/499: loss=5.870587424638375, w0=76.30000000000015, w1=16.522343746163596\n",
      "SubSGD iter. 335/499: loss=6.244153214006355, w0=77.00000000000016, w1=15.997710821451857\n",
      "SubSGD iter. 336/499: loss=6.544044777046754, w0=76.30000000000015, w1=15.20254109633451\n",
      "SubSGD iter. 337/499: loss=6.1377294131711215, w0=77.00000000000016, w1=15.217848763046646\n",
      "SubSGD iter. 338/499: loss=6.472820093867578, w0=76.30000000000015, w1=14.957136541664783\n",
      "SubSGD iter. 339/499: loss=6.134846443249018, w0=77.00000000000016, w1=11.583992864075105\n",
      "SubSGD iter. 340/499: loss=7.006628284445396, w0=76.30000000000015, w1=12.111181502822332\n",
      "SubSGD iter. 341/499: loss=6.5949558568516125, w0=75.60000000000015, w1=12.714947572788844\n",
      "SubSGD iter. 342/499: loss=6.201254335406835, w0=74.90000000000015, w1=12.083782232793991\n",
      "SubSGD iter. 343/499: loss=6.241598317760277, w0=75.60000000000015, w1=13.167657629485294\n",
      "SubSGD iter. 344/499: loss=6.103478178806043, w0=76.30000000000015, w1=14.180095967351715\n",
      "SubSGD iter. 345/499: loss=6.192640546150462, w0=77.00000000000016, w1=14.345548759284341\n",
      "SubSGD iter. 346/499: loss=6.478051863865858, w0=76.30000000000015, w1=14.89752019122202\n",
      "SubSGD iter. 347/499: loss=6.135970709205131, w0=75.60000000000015, w1=15.375568393648143\n",
      "SubSGD iter. 348/499: loss=5.8498393340115085, w0=74.90000000000015, w1=15.456382699136855\n",
      "SubSGD iter. 349/499: loss=5.619038738167157, w0=74.20000000000014, w1=14.509470437394425\n",
      "SubSGD iter. 350/499: loss=5.527707693329279, w0=73.50000000000014, w1=15.452976920931546\n",
      "SubSGD iter. 351/499: loss=5.369104109072884, w0=72.80000000000014, w1=14.752042923496793\n",
      "SubSGD iter. 352/499: loss=5.389983235773268, w0=72.10000000000014, w1=13.557304086392367\n",
      "SubSGD iter. 353/499: loss=5.712091380107087, w0=72.80000000000014, w1=13.722756878324994\n",
      "SubSGD iter. 354/499: loss=5.580417244141224, w0=73.50000000000014, w1=13.481776963658346\n",
      "SubSGD iter. 355/499: loss=5.637730318404346, w0=72.80000000000014, w1=13.283016097839822\n",
      "SubSGD iter. 356/499: loss=5.709649563066045, w0=73.50000000000014, w1=14.078185822957169\n",
      "SubSGD iter. 357/499: loss=5.49409423017733, w0=74.20000000000014, w1=12.86676293503099\n",
      "SubSGD iter. 358/499: loss=5.8824866765728325, w0=73.50000000000014, w1=13.306451340116174\n",
      "SubSGD iter. 359/499: loss=5.68997460021562, w0=74.20000000000014, w1=12.876119751655438\n",
      "SubSGD iter. 360/499: loss=5.8793660024009435, w0=74.90000000000015, w1=13.029013508770019\n",
      "SubSGD iter. 361/499: loss=5.941980558426747, w0=74.20000000000014, w1=12.605472255716004\n",
      "SubSGD iter. 362/499: loss=5.973887292132446, w0=73.50000000000014, w1=12.619227451691494\n",
      "SubSGD iter. 363/499: loss=5.939407302328674, w0=72.80000000000014, w1=12.354442342500725\n",
      "SubSGD iter. 364/499: loss=6.069693896316014, w0=72.10000000000014, w1=13.480374756001897\n",
      "SubSGD iter. 365/499: loss=5.736592871896921, w0=71.40000000000013, w1=14.050334880537882\n",
      "SubSGD iter. 366/499: loss=5.712424921106274, w0=70.70000000000013, w1=14.675150980348828\n",
      "SubSGD iter. 367/499: loss=5.751035090446968, w0=70.00000000000013, w1=14.79504279383716\n",
      "SubSGD iter. 368/499: loss=5.973539425767033, w0=70.70000000000013, w1=14.960849053404656\n",
      "SubSGD iter. 369/499: loss=5.6954950379155616, w0=71.40000000000013, w1=15.126655312972153\n",
      "SubSGD iter. 370/499: loss=5.488727175254587, w0=72.10000000000014, w1=15.40478228177384\n",
      "SubSGD iter. 371/499: loss=5.360991439026804, w0=71.40000000000013, w1=16.108287200164703\n",
      "SubSGD iter. 372/499: loss=5.43415830593896, w0=70.70000000000013, w1=16.997119024331198\n",
      "SubSGD iter. 373/499: loss=5.6685051082178, w0=71.40000000000013, w1=16.614993039780863\n",
      "SubSGD iter. 374/499: loss=5.451417716826242, w0=70.70000000000013, w1=16.894922032217327\n",
      "SubSGD iter. 375/499: loss=5.654767796523831, w0=70.00000000000013, w1=16.302604070949148\n",
      "SubSGD iter. 376/499: loss=5.8739912075284915, w0=69.30000000000013, w1=16.713679255491375\n",
      "SubSGD iter. 377/499: loss=6.218177626777028, w0=70.00000000000013, w1=16.681250247724016\n",
      "SubSGD iter. 378/499: loss=5.907574912082327, w0=69.30000000000013, w1=15.980316250289263\n",
      "SubSGD iter. 379/499: loss=6.189447518701643, w0=70.00000000000013, w1=15.540627845204078\n",
      "SubSGD iter. 380/499: loss=5.8682655322957, w0=69.30000000000013, w1=15.341866979385554\n",
      "SubSGD iter. 381/499: loss=6.21488735999857, w0=70.00000000000013, w1=15.96100037533022\n",
      "SubSGD iter. 382/499: loss=5.853758510376351, w0=70.70000000000013, w1=15.264415624089457\n",
      "SubSGD iter. 383/499: loss=5.647970634685723, w0=71.40000000000013, w1=14.92805266641338\n",
      "SubSGD iter. 384/499: loss=5.511713194851273, w0=72.10000000000014, w1=14.400864027666154\n",
      "SubSGD iter. 385/499: loss=5.49688257810667, w0=71.40000000000013, w1=13.206125190561728\n",
      "SubSGD iter. 386/499: loss=5.977152807442467, w0=72.10000000000014, w1=12.950693309460158\n",
      "SubSGD iter. 387/499: loss=5.9168887155646965, w0=71.40000000000013, w1=13.684807632461148\n",
      "SubSGD iter. 388/499: loss=5.820957203250096, w0=70.70000000000013, w1=14.18982536428547\n",
      "SubSGD iter. 389/499: loss=5.862526159788844, w0=71.40000000000013, w1=13.8142395259989\n",
      "SubSGD iter. 390/499: loss=5.781367012093536, w0=70.70000000000013, w1=14.655658035724844\n",
      "SubSGD iter. 391/499: loss=5.754980864297703, w0=71.40000000000013, w1=14.450824515849622\n",
      "SubSGD iter. 392/499: loss=5.60581178705545, w0=72.10000000000014, w1=14.720962966139291\n",
      "SubSGD iter. 393/499: loss=5.446229896855622, w0=72.80000000000014, w1=14.986446668363522\n",
      "SubSGD iter. 394/499: loss=5.362180934336403, w0=72.10000000000014, w1=15.011744433752364\n",
      "SubSGD iter. 395/499: loss=5.404460793687164, w0=72.80000000000014, w1=15.806914158869711\n",
      "SubSGD iter. 396/499: loss=5.313275586367142, w0=72.10000000000014, w1=16.419712329871125\n",
      "SubSGD iter. 397/499: loss=5.349345284466822, w0=71.40000000000013, w1=15.578522161646482\n",
      "SubSGD iter. 398/499: loss=5.44849725411247, w0=70.70000000000013, w1=14.372505871876035\n",
      "SubSGD iter. 399/499: loss=5.816936000388705, w0=71.40000000000013, w1=13.518659601450766\n",
      "SubSGD iter. 400/499: loss=5.873219723679413, w0=72.10000000000014, w1=14.042840152902484\n",
      "SubSGD iter. 401/499: loss=5.579932236057979, w0=72.80000000000014, w1=13.46983006988786\n",
      "SubSGD iter. 402/499: loss=5.65186263746724, w0=73.50000000000014, w1=14.112759768651657\n",
      "SubSGD iter. 403/499: loss=5.488061852723814, w0=72.80000000000014, w1=13.411825771216904\n",
      "SubSGD iter. 404/499: loss=5.668812617342623, w0=73.50000000000014, w1=13.57727856314953\n",
      "SubSGD iter. 405/499: loss=5.609342783745288, w0=72.80000000000014, w1=12.984960601881353\n",
      "SubSGD iter. 406/499: loss=5.813550574994242, w0=73.50000000000014, w1=11.947446408067812\n",
      "SubSGD iter. 407/499: loss=6.2138957469819776, w0=74.20000000000014, w1=11.36313387052192\n",
      "SubSGD iter. 408/499: loss=6.487289068717426, w0=73.50000000000014, w1=12.203161441981731\n",
      "SubSGD iter. 409/499: loss=6.107317101870397, w0=74.20000000000014, w1=11.94772956088016\n",
      "SubSGD iter. 410/499: loss=6.2332743330128295, w0=74.90000000000015, w1=12.15016645092116\n",
      "SubSGD iter. 411/499: loss=6.216407758793553, w0=74.20000000000014, w1=12.774982550732107\n",
      "SubSGD iter. 412/499: loss=5.914011972125177, w0=73.50000000000014, w1=11.8168474598303\n",
      "SubSGD iter. 413/499: loss=6.2706465233409086, w0=74.20000000000014, w1=12.161315706700767\n",
      "SubSGD iter. 414/499: loss=6.145557753576234, w0=73.50000000000014, w1=11.826087651397513\n",
      "SubSGD iter. 415/499: loss=6.266631268607074, w0=74.20000000000014, w1=12.037252045963717\n",
      "SubSGD iter. 416/499: loss=6.195852737001498, w0=74.90000000000015, w1=12.6511664593413\n",
      "SubSGD iter. 417/499: loss=6.047218756929562, w0=74.20000000000014, w1=13.469824829146928\n",
      "SubSGD iter. 418/499: loss=5.704329555219266, w0=73.50000000000014, w1=13.816908814766919\n",
      "SubSGD iter. 419/499: loss=5.546894268588602, w0=72.80000000000014, w1=12.869996553024489\n",
      "SubSGD iter. 420/499: loss=5.856410859142077, w0=72.10000000000014, w1=13.706035410297305\n",
      "SubSGD iter. 421/499: loss=5.667085080898538, w0=71.40000000000013, w1=13.825927223785637\n",
      "SubSGD iter. 422/499: loss=5.77785170430419, w0=72.10000000000014, w1=13.472811472218323\n",
      "SubSGD iter. 423/499: loss=5.7390267764009995, w0=72.80000000000014, w1=13.675248362259323\n",
      "SubSGD iter. 424/499: loss=5.593359925019468, w0=73.50000000000014, w1=12.935915562815378\n",
      "SubSGD iter. 425/499: loss=5.816897940225006, w0=74.20000000000014, w1=13.363432648032797\n",
      "SubSGD iter. 426/499: loss=5.729514220975513, w0=74.90000000000015, w1=13.979819596870046\n",
      "SubSGD iter. 427/499: loss=5.761646504771684, w0=74.20000000000014, w1=13.801467475490231\n",
      "SubSGD iter. 428/499: loss=5.633722064701314, w0=74.90000000000015, w1=13.062134676046288\n",
      "SubSGD iter. 429/499: loss=5.934250969864141, w0=74.20000000000014, w1=12.883782554666473\n",
      "SubSGD iter. 430/499: loss=5.8768286296174574, w0=74.90000000000015, w1=12.48780631144464\n",
      "SubSGD iter. 431/499: loss=6.097477768114454, w0=74.20000000000014, w1=12.50156150742013\n",
      "SubSGD iter. 432/499: loss=6.012759846246715, w0=73.50000000000014, w1=12.520140966319007\n",
      "SubSGD iter. 433/499: loss=5.978104766069556, w0=74.20000000000014, w1=13.163070665082802\n",
      "SubSGD iter. 434/499: loss=5.786809399067725, w0=74.90000000000015, w1=12.309224394657534\n",
      "SubSGD iter. 435/499: loss=6.159043190919207, w0=75.60000000000015, w1=12.83892536871965\n",
      "SubSGD iter. 436/499: loss=6.1734574923337995, w0=76.30000000000015, w1=13.035445451498152\n",
      "SubSGD iter. 437/499: loss=6.38073495192968, w0=75.60000000000015, w1=13.605405576034137\n",
      "SubSGD iter. 438/499: loss=6.025021670921174, w0=74.90000000000015, w1=13.406644710215613\n",
      "SubSGD iter. 439/499: loss=5.860670145599653, w0=74.20000000000014, w1=13.588508553790781\n",
      "SubSGD iter. 440/499: loss=5.678481317811231, w0=74.90000000000015, w1=13.967048337147514\n",
      "SubSGD iter. 441/499: loss=5.763489607510873, w0=74.20000000000014, w1=12.909133030319955\n",
      "SubSGD iter. 442/499: loss=5.868534642601225, w0=74.90000000000015, w1=12.668153115653308\n",
      "SubSGD iter. 443/499: loss=6.04204488749908, w0=74.20000000000014, w1=13.852155316047199\n",
      "SubSGD iter. 444/499: loss=5.6241886580421605, w0=73.50000000000014, w1=14.291843721132384\n",
      "SubSGD iter. 445/499: loss=5.460349790985753, w0=74.20000000000014, w1=13.90971773658205\n",
      "SubSGD iter. 446/499: loss=5.613593841752706, w0=74.90000000000015, w1=13.877288728814689\n",
      "SubSGD iter. 447/499: loss=5.776443394653336, w0=74.20000000000014, w1=14.573873480055452\n",
      "SubSGD iter. 448/499: loss=5.520094564881278, w0=73.50000000000014, w1=14.949459318342022\n",
      "SubSGD iter. 449/499: loss=5.393537874482875, w0=72.80000000000014, w1=13.754720481237596\n",
      "SubSGD iter. 450/499: loss=5.572508543184299, w0=73.50000000000014, w1=13.99903320494673\n",
      "SubSGD iter. 451/499: loss=5.508805508237287, w0=74.20000000000014, w1=15.124163165077038\n",
      "SubSGD iter. 452/499: loss=5.4787047991199795, w0=73.50000000000014, w1=15.519599957020043\n",
      "SubSGD iter. 453/499: loss=5.366776726021308, w0=74.20000000000014, w1=15.169662025539726\n",
      "SubSGD iter. 454/499: loss=5.477086471750229, w0=74.90000000000015, w1=15.964831750657073\n",
      "SubSGD iter. 455/499: loss=5.63578036671025, w0=74.20000000000014, w1=16.469849482481393\n",
      "SubSGD iter. 456/499: loss=5.509314903321969, w0=73.50000000000014, w1=16.900860167230007\n",
      "SubSGD iter. 457/499: loss=5.435742863941937, w0=74.20000000000014, w1=17.54700740061723\n",
      "SubSGD iter. 458/499: loss=5.66604267642554, w0=73.50000000000014, w1=16.998612896859417\n",
      "SubSGD iter. 459/499: loss=5.451202165013557, w0=74.20000000000014, w1=17.343081143729883\n",
      "SubSGD iter. 460/499: loss=5.623642569490391, w0=73.50000000000014, w1=17.851582261469716\n",
      "SubSGD iter. 461/499: loss=5.610641548234672, w0=72.80000000000014, w1=17.150648264034963\n",
      "SubSGD iter. 462/499: loss=5.418717262145823, w0=72.10000000000014, w1=17.394675088744115\n",
      "SubSGD iter. 463/499: loss=5.4669370216664435, w0=71.40000000000013, w1=17.16555339645736\n",
      "SubSGD iter. 464/499: loss=5.503479409238428, w0=70.70000000000013, w1=16.90076828726659\n",
      "SubSGD iter. 465/499: loss=5.655511480752914, w0=70.00000000000013, w1=16.702007421448066\n",
      "SubSGD iter. 466/499: loss=5.909473989166367, w0=70.70000000000013, w1=16.27167583298733\n",
      "SubSGD iter. 467/499: loss=5.611075813122873, w0=71.40000000000013, w1=16.381975866448123\n",
      "SubSGD iter. 468/499: loss=5.440915036203033, w0=72.10000000000014, w1=16.54742865838075\n",
      "SubSGD iter. 469/499: loss=5.35592614603877, w0=72.80000000000014, w1=17.279393251888383\n",
      "SubSGD iter. 470/499: loss=5.438806726421242, w0=72.10000000000014, w1=15.916641856496636\n",
      "SubSGD iter. 471/499: loss=5.339814908770529, w0=72.80000000000014, w1=15.520665613274803\n",
      "SubSGD iter. 472/499: loss=5.3270533236558375, w0=73.50000000000014, w1=14.993476974527576\n",
      "SubSGD iter. 473/499: loss=5.390560053751089, w0=74.20000000000014, w1=15.639624207914796\n",
      "SubSGD iter. 474/499: loss=5.465714794240769, w0=73.50000000000014, w1=14.583222976535906\n",
      "SubSGD iter. 475/499: loss=5.425214847840725, w0=74.20000000000014, w1=14.779743059314407\n",
      "SubSGD iter. 476/499: loss=5.5001605539179925, w0=74.90000000000015, w1=15.422672758078203\n",
      "SubSGD iter. 477/499: loss=5.618539466838028, w0=74.20000000000014, w1=14.581482589853561\n",
      "SubSGD iter. 478/499: loss=5.519223409536056, w0=73.50000000000014, w1=14.96676673018169\n",
      "SubSGD iter. 479/499: loss=5.392197217476453, w0=72.80000000000014, w1=13.79843592966422\n",
      "SubSGD iter. 480/499: loss=5.5623808223839815, w0=73.50000000000014, w1=13.24432885566764\n",
      "SubSGD iter. 481/499: loss=5.7092172522010385, w0=72.80000000000014, w1=13.59141284128763\n",
      "SubSGD iter. 482/499: loss=5.616734172551415, w0=72.10000000000014, w1=14.126932524113835\n",
      "SubSGD iter. 483/499: loss=5.558172182076052, w0=71.40000000000013, w1=14.53800770865606\n",
      "SubSGD iter. 484/499: loss=5.584759475708863, w0=72.10000000000014, w1=14.816134677457748\n",
      "SubSGD iter. 485/499: loss=5.431732680647341, w0=71.40000000000013, w1=15.321152409282071\n",
      "SubSGD iter. 486/499: loss=5.4685075473791915, w0=70.70000000000013, w1=14.415977306095007\n",
      "SubSGD iter. 487/499: loss=5.8069494584955, w0=70.00000000000013, w1=15.09657648375109\n",
      "SubSGD iter. 488/499: loss=5.927593393787764, w0=70.70000000000013, w1=14.700600240529257\n",
      "SubSGD iter. 489/499: loss=5.745883635534562, w0=71.40000000000013, w1=14.91176463509546\n",
      "SubSGD iter. 490/499: loss=5.513609822186671, w0=72.10000000000014, w1=14.338754552080838\n",
      "SubSGD iter. 491/499: loss=5.509047161860014, w0=71.40000000000013, w1=14.36405231746968\n",
      "SubSGD iter. 492/499: loss=5.628355864829427, w0=72.10000000000014, w1=15.197389900029258\n",
      "SubSGD iter. 493/499: loss=5.37984307806644, w0=71.40000000000013, w1=15.582674040357386\n",
      "SubSGD iter. 494/499: loss=5.448306371227331, w0=70.70000000000013, w1=16.20546044717768\n",
      "SubSGD iter. 495/499: loss=5.610947071359188, w0=70.00000000000013, w1=16.75743187911536\n",
      "SubSGD iter. 496/499: loss=5.914544781503055, w0=69.30000000000013, w1=16.87732369260369\n",
      "SubSGD iter. 497/499: loss=6.233805903245483, w0=70.00000000000013, w1=17.520253391367483\n",
      "SubSGD iter. 498/499: loss=6.0083047655871695, w0=69.30000000000013, w1=17.943492816139997\n",
      "SubSGD iter. 499/499: loss=6.3902209789806745, w0=70.00000000000013, w1=18.562626212084663\n",
      "SubSGD: execution time=0.030 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-05T08:19:49.910445605Z",
     "start_time": "2023-10-05T08:19:49.579528262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccf5ed1e99c9405c9adca5b737ee7b83"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<function __main__.plot_figure(n_iter)>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
